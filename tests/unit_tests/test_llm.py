#!/usr/bin/env python3
"""
LLM ì‹œìŠ¤í…œ ê¸°ë³¸ í…ŒìŠ¤íŠ¸
"""
import sys
import os
import pytest

# ëª¨ë“ˆ ê²½ë¡œ ì¶”ê°€ (ë£¨íŠ¸ ë””ë ‰í† ë¦¬ ê¸°ì¤€)
sys.path.insert(0, os.path.join(os.path.dirname(__file__), "..", ".."))

from newsletter.llm_factory import (
    get_available_providers,
    get_provider_info,
    get_llm_for_task,
)
from newsletter import config


class TestLLMSystem:
    """LLM ì‹œìŠ¤í…œ ê¸°ë³¸ í…ŒìŠ¤íŠ¸ í´ë˜ìŠ¤"""

    def test_api_keys_configuration(self):
        """API í‚¤ ì„¤ì • ìƒíƒœë¥¼ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤."""
        print("\n=== API í‚¤ ìƒíƒœ í…ŒìŠ¤íŠ¸ ===")

        # GeminiëŠ” í•„ìˆ˜ì´ë¯€ë¡œ ë°˜ë“œì‹œ ì„¤ì •ë˜ì–´ì•¼ í•¨
        assert config.GEMINI_API_KEY is not None, "GEMINI_API_KEYëŠ” í•„ìˆ˜ì…ë‹ˆë‹¤"
        print(f"âœ… GEMINI_API_KEY: ì„¤ì •ë¨")

        # ë‹¤ë¥¸ API í‚¤ë“¤ì€ ì„ íƒì‚¬í•­ì´ì§€ë§Œ ìƒíƒœ í™•ì¸
        openai_status = "ì„¤ì •ë¨" if config.OPENAI_API_KEY else "ë¯¸ì„¤ì •"
        anthropic_status = "ì„¤ì •ë¨" if config.ANTHROPIC_API_KEY else "ë¯¸ì„¤ì •"

        print(f"ğŸ”§ OPENAI_API_KEY: {openai_status}")
        print(f"ğŸ”§ ANTHROPIC_API_KEY: {anthropic_status}")

    def test_provider_availability(self):
        """LLM ì œê³µì ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ë¥¼ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤."""
        print("\n=== ì œê³µì ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í…ŒìŠ¤íŠ¸ ===")

        available_providers = get_available_providers()

        # ìµœì†Œí•œ í•˜ë‚˜ì˜ ì œê³µìëŠ” ì‚¬ìš© ê°€ëŠ¥í•´ì•¼ í•¨
        assert len(available_providers) > 0, "ì‚¬ìš© ê°€ëŠ¥í•œ LLM ì œê³µìê°€ ì—†ìŠµë‹ˆë‹¤"

        # GeminiëŠ” ë°˜ë“œì‹œ ì‚¬ìš© ê°€ëŠ¥í•´ì•¼ í•¨
        assert "gemini" in available_providers, "Gemini ì œê³µìê°€ ì‚¬ìš© ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤"

        print(f"âœ… ì‚¬ìš© ê°€ëŠ¥í•œ ì œê³µì: {available_providers}")

        # ì œê³µìë³„ ìƒì„¸ ì •ë³´ í™•ì¸
        provider_info = get_provider_info()
        for provider, info in provider_info.items():
            status = "âœ…" if info["available"] else "âŒ"
            print(f"{status} {provider.upper()}: ì‚¬ìš© ê°€ëŠ¥ = {info['available']}")

    def test_llm_config_validation(self):
        """LLM ì„¤ì • êµ¬ì¡°ë¥¼ ê²€ì¦í•©ë‹ˆë‹¤."""
        print("\n=== LLM ì„¤ì • êµ¬ì¡° í…ŒìŠ¤íŠ¸ ===")

        # ê¸°ë³¸ ì„¤ì • í™•ì¸
        assert "default_provider" in config.LLM_CONFIG, "ê¸°ë³¸ ì œê³µì ì„¤ì •ì´ ì—†ìŠµë‹ˆë‹¤"
        default_provider = config.LLM_CONFIG["default_provider"]
        print(f"âœ… ê¸°ë³¸ ì œê³µì: {default_provider}")

        # ëª¨ë¸ ì„¤ì • í™•ì¸
        assert "models" in config.LLM_CONFIG, "ëª¨ë¸ ì„¤ì •ì´ ì—†ìŠµë‹ˆë‹¤"
        models = config.LLM_CONFIG["models"]

        # í•„ìˆ˜ ì‘ì—…ë“¤ì´ ì„¤ì •ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
        required_tasks = [
            "keyword_generation",
            "theme_extraction",
            "news_summarization",
            "html_generation",
        ]

        for task in required_tasks:
            assert task in models, f"í•„ìˆ˜ ì‘ì—… '{task}' ì„¤ì •ì´ ì—†ìŠµë‹ˆë‹¤"
            task_config = models[task]
            assert "provider" in task_config, f"ì‘ì—… '{task}'ì— ì œê³µì ì„¤ì •ì´ ì—†ìŠµë‹ˆë‹¤"
            assert "model" in task_config, f"ì‘ì—… '{task}'ì— ëª¨ë¸ ì„¤ì •ì´ ì—†ìŠµë‹ˆë‹¤"
            print(f"âœ… {task}: {task_config['provider']} / {task_config['model']}")

    def test_llm_instance_creation(self):
        """LLM ì¸ìŠ¤í„´ìŠ¤ ìƒì„±ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤."""
        print("\n=== LLM ì¸ìŠ¤í„´ìŠ¤ ìƒì„± í…ŒìŠ¤íŠ¸ ===")

        # ì—¬ëŸ¬ ì‘ì—…ì— ëŒ€í•´ LLM ìƒì„± í…ŒìŠ¤íŠ¸
        test_tasks = ["theme_extraction", "news_summarization", "html_generation"]

        for task in test_tasks:
            try:
                llm = get_llm_for_task(task)
                assert llm is not None, f"ì‘ì—… '{task}'ì— ëŒ€í•œ LLM ìƒì„± ì‹¤íŒ¨"
                print(f"âœ… {task}: {type(llm).__name__} ìƒì„± ì„±ê³µ")
            except Exception as e:
                pytest.fail(f"ì‘ì—… '{task}'ì— ëŒ€í•œ LLM ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")

    def test_fallback_mechanism(self):
        """Fallback ë©”ì»¤ë‹ˆì¦˜ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤."""
        print("\n=== Fallback ë©”ì»¤ë‹ˆì¦˜ í…ŒìŠ¤íŠ¸ ===")

        # ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ì‘ì—…ì— ëŒ€í•œ ê¸°ë³¸ LLM ìƒì„± í…ŒìŠ¤íŠ¸
        try:
            llm = get_llm_for_task("nonexistent_task")
            assert llm is not None, "Fallback LLM ìƒì„± ì‹¤íŒ¨"
            print(f"âœ… Fallback í…ŒìŠ¤íŠ¸: {type(llm).__name__} ìƒì„± ì„±ê³µ")
        except Exception as e:
            pytest.fail(f"Fallback ë©”ì»¤ë‹ˆì¦˜ í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜: {e}")


def test_suite_runner():
    """í…ŒìŠ¤íŠ¸ ìŠ¤ìœ„íŠ¸ë¥¼ ì§ì ‘ ì‹¤í–‰í•˜ëŠ” í•¨ìˆ˜"""
    print("=== LLM ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ìŠ¤ìœ„íŠ¸ ===")
    print(f"í˜„ì¬ ë””ë ‰í† ë¦¬: {os.getcwd()}")

    test_instance = TestLLMSystem()

    try:
        test_instance.test_api_keys_configuration()
        test_instance.test_provider_availability()
        test_instance.test_llm_config_validation()
        test_instance.test_llm_instance_creation()
        test_instance.test_fallback_mechanism()

        print("\nğŸ‰ ëª¨ë“  ê¸°ë³¸ í…ŒìŠ¤íŠ¸ í†µê³¼!")
        return True

    except Exception as e:
        print(f"\nâŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        import traceback

        traceback.print_exc()
        return False


if __name__ == "__main__":
    # ì§ì ‘ ì‹¤í–‰ ì‹œ í…ŒìŠ¤íŠ¸ ìˆ˜í–‰
    test_suite_runner()
