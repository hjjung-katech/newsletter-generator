#!/usr/bin/env python3
"""
LLM ì œê³µìë³„ ìƒì„¸ í…ŒìŠ¤íŠ¸
"""
import sys
import os
import pytest
import time

# ëª¨ë“ˆ ê²½ë¡œ ì¶”ê°€ (ë£¨íŠ¸ ë””ë ‰í† ë¦¬ ê¸°ì¤€)
sys.path.insert(0, os.path.join(os.path.dirname(__file__), "..", ".."))

from newsletter.llm_factory import get_llm_for_task, llm_factory
from newsletter import config


class TestLLMProviders:
    """LLM ì œê³µìë³„ ìƒì„¸ í…ŒìŠ¤íŠ¸ í´ë˜ìŠ¤"""

    # í…ŒìŠ¤íŠ¸í•  ì‘ì—…ë“¤ê³¼ ì„¤ëª…
    TASKS = [
        ("keyword_generation", "í‚¤ì›Œë“œ ìƒì„±", "ì°½ì˜ì  ì‘ì—…"),
        ("theme_extraction", "í…Œë§ˆ ì¶”ì¶œ", "ë¹ ë¥¸ ë¶„ì„"),
        ("news_summarization", "ë‰´ìŠ¤ ìš”ì•½", "ì •í™•í•œ ìš”ì•½"),
        ("section_regeneration", "ì„¹ì…˜ ì¬ìƒì„±", "êµ¬ì¡°í™” ì‘ì—…"),
        ("introduction_generation", "ì†Œê°œ ìƒì„±", "ìì—°ìŠ¤ëŸ¬ìš´ ê¸€ì“°ê¸°"),
        ("html_generation", "HTML ìƒì„±", "ë³µì¡í•œ êµ¬ì¡°í™”"),
        ("article_scoring", "ê¸°ì‚¬ ì ìˆ˜", "ë¹ ë¥¸ íŒë‹¨"),
        ("translation", "ë²ˆì—­", "ì •í™•ì„±"),
    ]

    def test_all_task_llm_creation(self):
        """ëª¨ë“  ì‘ì—…ì— ëŒ€í•œ LLM ìƒì„±ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤."""
        print("\n=== ì „ì²´ ì‘ì—… LLM ìƒì„± í…ŒìŠ¤íŠ¸ ===")

        created_llms = {}
        failed_tasks = []

        for task_id, task_name, description in self.TASKS:
            try:
                llm = get_llm_for_task(task_id)
                assert llm is not None, f"ì‘ì—… '{task_id}'ì— ëŒ€í•œ LLMì´ Noneì…ë‹ˆë‹¤"

                provider_type = type(llm).__name__
                created_llms[task_id] = {
                    "llm": llm,
                    "type": provider_type,
                    "task_name": task_name,
                    "description": description,
                }
                print(f"âœ… {task_name} ({task_id}): {provider_type}")

            except Exception as e:
                failed_tasks.append((task_id, task_name, str(e)))
                print(f"âŒ {task_name} ({task_id}): ì‹¤íŒ¨ - {e}")

        # ì‹¤íŒ¨í•œ ì‘ì—…ì´ ìˆìœ¼ë©´ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨
        if failed_tasks:
            failure_msg = "\n".join(
                [f"- {name}: {error}" for _, name, error in failed_tasks]
            )
            pytest.fail(f"ë‹¤ìŒ ì‘ì—…ë“¤ì˜ LLM ìƒì„±ì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤:\n{failure_msg}")

        return created_llms

    @pytest.mark.real_api
    def test_llm_response_quality(self):
        """ê° LLMì˜ ì‘ë‹µ í’ˆì§ˆì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤."""
        print("\n=== LLM ì‘ë‹µ í’ˆì§ˆ í…ŒìŠ¤íŠ¸ ===")

        # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ë“¤
        test_prompts = {
            "keyword_generation": "ìŠ¤ë§ˆíŠ¸íŒ©í† ë¦¬ ë„ë©”ì¸ì˜ í‚¤ì›Œë“œ 3ê°œë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”",
            "theme_extraction": "AI, ë¨¸ì‹ ëŸ¬ë‹, ë”¥ëŸ¬ë‹ì—ì„œ ê³µí†µ í…Œë§ˆë¥¼ ì°¾ì•„ì£¼ì„¸ìš”",
            "news_summarization": "ì•ˆë…•í•˜ì„¸ìš”. ì´ ë©”ì‹œì§€ë¥¼ í•œ ì¤„ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”.",
            "translation": "Hello, how are you today?ë¥¼ í•œêµ­ì–´ë¡œ ë²ˆì—­í•´ì£¼ì„¸ìš”",
        }

        response_results = {}

        for task_id in test_prompts:
            if task_id in test_prompts:
                try:
                    llm = get_llm_for_task(task_id)
                    start_time = time.time()
                    response = llm.invoke(test_prompts[task_id])
                    response_time = time.time() - start_time

                    response_text = str(response).strip()

                    # ì‘ë‹µ í’ˆì§ˆ ê²€ì¦
                    assert len(response_text) > 0, f"{task_id}: ë¹ˆ ì‘ë‹µ"
                    assert len(response_text) > 10, f"{task_id}: ì‘ë‹µì´ ë„ˆë¬´ ì§§ìŒ"
                    assert (
                        response_time < 60
                    ), f"{task_id}: ì‘ë‹µ ì‹œê°„ ì´ˆê³¼ ({response_time:.2f}ì´ˆ)"

                    response_results[task_id] = {
                        "length": len(response_text),
                        "time": response_time,
                        "type": type(llm).__name__,
                    }

                    print(
                        f"âœ… {task_id}: {response_time:.2f}ì´ˆ, {len(response_text)}ì ({type(llm).__name__})"
                    )

                except Exception as e:
                    print(f"âŒ {task_id}: ì‘ë‹µ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨ - {e}")
                    pytest.fail(f"ì‘ì—… '{task_id}'ì˜ ì‘ë‹µ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")

        return response_results

    def test_provider_distribution(self):
        """ì œê³µìë³„ ì‘ì—… ë¶„ë°°ê°€ ì ì ˆí•œì§€ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤."""
        print("\n=== ì œê³µìë³„ ì‘ì—… ë¶„ë°° í…ŒìŠ¤íŠ¸ ===")

        provider_usage = {}

        for task_id, task_name, _ in self.TASKS:
            try:
                llm = get_llm_for_task(task_id)
                provider_type = type(llm).__name__

                if provider_type not in provider_usage:
                    provider_usage[provider_type] = []
                provider_usage[provider_type].append(task_name)

            except Exception as e:
                print(f"âŒ {task_name}: ì œê³µì í™•ì¸ ì‹¤íŒ¨ - {e}")

        # ê²°ê³¼ ì¶œë ¥
        for provider, tasks in provider_usage.items():
            print(f"ğŸ¤– {provider}: {len(tasks)}ê°œ ì‘ì—…")
            for task in tasks:
                print(f"   - {task}")

        # ìµœì†Œí•œ í•˜ë‚˜ì˜ ì œê³µìëŠ” ì‚¬ìš©ë˜ì–´ì•¼ í•¨
        assert len(provider_usage) > 0, "ì‚¬ìš©ëœ ì œê³µìê°€ ì—†ìŠµë‹ˆë‹¤"

        return provider_usage

    def test_fallback_mechanism_detailed(self):
        """ìƒì„¸í•œ Fallback ë©”ì»¤ë‹ˆì¦˜ í…ŒìŠ¤íŠ¸"""
        print("\n=== ìƒì„¸ Fallback ë©”ì»¤ë‹ˆì¦˜ í…ŒìŠ¤íŠ¸ ===")

        # ì›ë³¸ ì„¤ì • ë°±ì—…
        original_config = config.LLM_CONFIG.copy()

        try:
            # ì˜ëª»ëœ ì„¤ì •ìœ¼ë¡œ í…ŒìŠ¤íŠ¸
            test_scenarios = [
                {
                    "name": "ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ì œê³µì",
                    "config": {
                        "provider": "nonexistent_provider",
                        "model": "nonexistent_model",
                        "temperature": 0.5,
                    },
                },
                {
                    "name": "ì˜ëª»ëœ ëª¨ë¸ëª…",
                    "config": {
                        "provider": "gemini",
                        "model": "nonexistent_gemini_model",
                        "temperature": 0.5,
                    },
                },
            ]

            for scenario in test_scenarios:
                print(f"\nğŸ§ª í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤: {scenario['name']}")

                # ì„ì‹œ ì„¤ì • ì ìš©
                test_config = original_config.copy()
                test_config["models"] = {"test_fallback_task": scenario["config"]}
                config.LLM_CONFIG = test_config

                try:
                    llm = get_llm_for_task("test_fallback_task")
                    assert (
                        llm is not None
                    ), f"Fallback LLMì´ ìƒì„±ë˜ì§€ ì•ŠìŒ: {scenario['name']}"
                    print(f"âœ… {scenario['name']}: {type(llm).__name__} Fallback ì„±ê³µ")

                except Exception as e:
                    pytest.fail(f"Fallback ì‹¤íŒ¨ - {scenario['name']}: {e}")

        finally:
            # ì›ë³¸ ì„¤ì • ë³µì›
            config.LLM_CONFIG = original_config

    @pytest.mark.real_api
    def test_performance_benchmarks(self):
        """ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸"""
        print("\n=== ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸ ===")

        # ë¹ ë¥¸ ì‘ì—…ê³¼ ëŠë¦° ì‘ì—… êµ¬ë¶„
        fast_tasks = ["theme_extraction", "article_scoring"]  # Flash ëª¨ë¸ ì‚¬ìš©
        slow_tasks = ["news_summarization", "html_generation"]  # Pro ëª¨ë¸ ì‚¬ìš©

        performance_results = {}

        test_prompt = "ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ ë©”ì‹œì§€ì…ë‹ˆë‹¤."

        for task_group, expected_speed in [("fast", fast_tasks), ("slow", slow_tasks)]:
            group_times = []

            for task_id in expected_speed:
                if any(t[0] == task_id for t in self.TASKS):
                    try:
                        llm = get_llm_for_task(task_id)
                        start_time = time.time()
                        response = llm.invoke(test_prompt)
                        response_time = time.time() - start_time

                        group_times.append(response_time)
                        print(f"ğŸ“Š {task_id}: {response_time:.2f}ì´ˆ")

                    except Exception as e:
                        print(f"âŒ {task_id}: ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨ - {e}")

            if group_times:
                avg_time = sum(group_times) / len(group_times)
                performance_results[task_group] = {
                    "average_time": avg_time,
                    "times": group_times,
                }
                print(f"ğŸ“ˆ {task_group.upper()} ì‘ì—… í‰ê·  ì‹œê°„: {avg_time:.2f}ì´ˆ")

        # ì„±ëŠ¥ ê²€ì¦: fast ì‘ì—…ì´ slow ì‘ì—…ë³´ë‹¤ ë¹¨ë¼ì•¼ í•¨ (ì¼ë°˜ì ìœ¼ë¡œ)
        if "fast" in performance_results and "slow" in performance_results:
            fast_avg = performance_results["fast"]["average_time"]
            slow_avg = performance_results["slow"]["average_time"]
            print(f"ğŸƒ ë¹ ë¥¸ ì‘ì—… vs ì •í™•í•œ ì‘ì—…: {fast_avg:.2f}ì´ˆ vs {slow_avg:.2f}ì´ˆ")

        return performance_results


@pytest.mark.real_api
def test_comprehensive_suite():
    """ì¢…í•© í…ŒìŠ¤íŠ¸ ìŠ¤ìœ„íŠ¸ ì‹¤í–‰"""
    print("=== LLM ì œê³µìë³„ ì¢…í•© í…ŒìŠ¤íŠ¸ ìŠ¤ìœ„íŠ¸ ===")
    print(f"í˜„ì¬ ë””ë ‰í† ë¦¬: {os.getcwd()}")

    test_instance = TestLLMProviders()
    results = {}

    try:
        print("\n1ï¸âƒ£ LLM ìƒì„± í…ŒìŠ¤íŠ¸")
        results["llm_creation"] = test_instance.test_all_task_llm_creation()

        print("\n2ï¸âƒ£ ì‘ë‹µ í’ˆì§ˆ í…ŒìŠ¤íŠ¸")
        results["response_quality"] = test_instance.test_llm_response_quality()

        print("\n3ï¸âƒ£ ì œê³µì ë¶„ë°° í…ŒìŠ¤íŠ¸")
        results["provider_distribution"] = test_instance.test_provider_distribution()

        print("\n4ï¸âƒ£ Fallback ë©”ì»¤ë‹ˆì¦˜ í…ŒìŠ¤íŠ¸")
        test_instance.test_fallback_mechanism_detailed()

        print("\n5ï¸âƒ£ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸")
        results["performance"] = test_instance.test_performance_benchmarks()

        print("\nğŸ‰ ëª¨ë“  ìƒì„¸ í…ŒìŠ¤íŠ¸ í†µê³¼!")
        return results

    except Exception as e:
        print(f"\nâŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        import traceback

        traceback.print_exc()
        return None


if __name__ == "__main__":
    # ì§ì ‘ ì‹¤í–‰ ì‹œ ì¢…í•© í…ŒìŠ¤íŠ¸ ìˆ˜í–‰
    test_comprehensive_suite()
