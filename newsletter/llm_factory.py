"""
LLM Factory Module

ì´ ëª¨ë“ˆì€ ë‹¤ì–‘í•œ LLM ì œê³µì(Gemini, OpenAI, Anthropic)ë¥¼ í†µí•© ê´€ë¦¬í•˜ê³ ,
ê¸°ëŠ¥ë³„ë¡œ ìµœì í™”ëœ LLM ëª¨ë¸ì„ ì œê³µí•˜ëŠ” íŒ©í† ë¦¬ í´ë˜ìŠ¤ë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤.

F-14: ì¤‘ì•™í™”ëœ ì„¤ì •ì„ í™œìš©í•œ ì„±ëŠ¥ ìµœì í™”
"""

import os
import time
from abc import ABC, abstractmethod
from typing import Any, Dict, List, Optional

from .utils.logger import get_logger
from .utils.error_handling import handle_exception

# ì¤‘ì•™í™”ëœ ì„¤ì • import ì¶”ê°€ (F-14)
try:
    from .centralized_settings import get_settings

    CENTRALIZED_SETTINGS_AVAILABLE = True
except ImportError:
    CENTRALIZED_SETTINGS_AVAILABLE = False
    get_settings = None

# ë¡œê±° ì´ˆê¸°í™”
logger = get_logger()


def validate_api_keys():
    """
    ì‹œì‘ ì‹œ ëª¨ë“  í•„ìš”í•œ API í‚¤ì˜ ìœ íš¨ì„±ì„ ê²€ì‚¬í•©ë‹ˆë‹¤.
    """
    logger.info("ğŸ” API í‚¤ ìœ íš¨ì„± ê²€ì‚¬ ì‹œì‘...")

    api_key_checks = {
        "gemini": ("GEMINI_API_KEY", "Gemini API"),
        "openai": ("OPENAI_API_KEY", "OpenAI API"),
        "anthropic": ("ANTHROPIC_API_KEY", "Anthropic API"),
        "serper": ("SERPER_API_KEY", "Serper API"),
    }

    available_providers = []
    missing_keys = []
    invalid_keys = []

    for provider, (env_var, api_name) in api_key_checks.items():
        api_key = os.getenv(env_var)

        if not api_key:
            missing_keys.append(f"{api_name} ({env_var})")
            logger.warning(f"âŒ {api_name} í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•ŠìŒ: {env_var}")
        elif api_key.startswith("your-") or api_key == "your-openai-api-key":
            invalid_keys.append(f"{api_name} ({env_var}) - í”Œë ˆì´ìŠ¤í™€ë” ê°’")
            logger.error(f"âŒ {api_name} í‚¤ê°€ í”Œë ˆì´ìŠ¤í™€ë” ê°’: {env_var}")
        else:
            available_providers.append(provider)
            logger.info(f"âœ… {api_name} í‚¤ í™•ì¸ë¨: {env_var}")

    # ìµœì†Œ í•˜ë‚˜ì˜ LLM ì œê³µìê°€ í•„ìš”
    llm_providers = [
        p for p in available_providers if p in ["gemini", "openai", "anthropic"]
    ]
    if not llm_providers:
        logger.error("âŒ ì‚¬ìš© ê°€ëŠ¥í•œ LLM ì œê³µìê°€ ì—†ìŠµë‹ˆë‹¤!")
        logger.error("ë‹¤ìŒ ì¤‘ í•˜ë‚˜ ì´ìƒì˜ API í‚¤ë¥¼ ì„¤ì •í•´ì•¼ í•©ë‹ˆë‹¤:")
        for provider in ["gemini", "openai", "anthropic"]:
            env_var = api_key_checks[provider][0]
            logger.error(f"  - {env_var}")
        raise ValueError("ì‚¬ìš© ê°€ëŠ¥í•œ LLM ì œê³µìê°€ ì—†ìŠµë‹ˆë‹¤. API í‚¤ë¥¼ í™•ì¸í•˜ì„¸ìš”.")

    # Serper API í‚¤ëŠ” ë‰´ìŠ¤ ê²€ìƒ‰ì— í•„ìš”
    if "serper" not in available_providers:
        logger.warning("âš ï¸ Serper API í‚¤ê°€ ì—†ì–´ ë‰´ìŠ¤ ê²€ìƒ‰ ê¸°ëŠ¥ì´ ì œí•œë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

    logger.info(f"âœ… API í‚¤ ê²€ì‚¬ ì™„ë£Œ. ì‚¬ìš© ê°€ëŠ¥í•œ LLM: {', '.join(llm_providers)}")
    return available_providers


# Google Cloud ì¸ì¦ ë¬¸ì œ í•´ê²°
# ì‹œìŠ¤í…œì— ì˜ëª»ëœ GOOGLE_APPLICATION_CREDENTIALSê°€ ì„¤ì •ë˜ì–´ ìˆëŠ” ê²½ìš° ì²˜ë¦¬
google_creds_path = os.environ.get("GOOGLE_APPLICATION_CREDENTIALS", "")
if google_creds_path and not os.path.exists(google_creds_path):
    # íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë©´ í™˜ê²½ë³€ìˆ˜ ì œê±°
    logger.warning(
        f"GOOGLE_APPLICATION_CREDENTIALSì´ ì¡´ì¬í•˜ì§€ ì•ŠëŠ” íŒŒì¼ì„ ê°€ë¦¬í‚µë‹ˆë‹¤: {google_creds_path}"
    )
    logger.info("Google Cloud ì¸ì¦ì„ ë¹„í™œì„±í™”í•˜ê³  API í‚¤ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    os.environ.pop("GOOGLE_APPLICATION_CREDENTIALS", None)

# Google Cloud ê¸°ë³¸ ì¸ì¦ íŒŒì¼ ê²€ìƒ‰ ì™„ì „ ë¹„í™œì„±í™”
# ì´ë ‡ê²Œ í•˜ë©´ langchain_google_genaiê°€ credentials.jsonì„ ì°¾ì§€ ì•ŠìŒ
os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = ""
os.environ["GOOGLE_CLOUD_PROJECT"] = ""

# Google Cloud SDK ê¸°ë³¸ ì„¤ì • íŒŒì¼ë„ ë¹„í™œì„±í™”
os.environ.pop("CLOUDSDK_CONFIG", None)
os.environ.pop("GCLOUD_PROJECT", None)

try:
    from langchain_google_genai import ChatGoogleGenerativeAI
except ImportError:
    ChatGoogleGenerativeAI = None

try:
    from langchain_openai import ChatOpenAI
except ImportError:
    ChatOpenAI = None

try:
    from langchain_anthropic import ChatAnthropic
except ImportError:
    ChatAnthropic = None

# LangChain Runnable ê´€ë ¨ imports ì¶”ê°€
try:
    from langchain_core.runnables import Runnable
    from langchain_core.runnables.config import RunnableConfig
    from langchain_core.runnables.utils import Input, Output
except ImportError:
    # Fallback for older versions
    Runnable = object
    RunnableConfig = dict
    Input = Any
    Output = Any

from . import config
from .cost_tracking import get_cost_callback_for_provider


class QuotaExceededError(Exception):
    """API í• ë‹¹ëŸ‰ ì´ˆê³¼ ì—ëŸ¬"""

    pass


class LLMWithFallback(Runnable):
    """
    API í• ë‹¹ëŸ‰ ì´ˆê³¼ ì‹œ ìë™ìœ¼ë¡œ ë‹¤ë¥¸ ì œê³µìë¡œ fallbackí•˜ëŠ” LLM ë˜í¼
    F-14: ì¤‘ì•™í™”ëœ ì„¤ì • í™œìš©
    """

    def __init__(self, primary_llm, factory, task, callbacks=None):
        """F-14 ì¤‘ì•™í™”ëœ ì„¤ì •ì„ ì‚¬ìš©í•œ LLM with Fallback ì´ˆê¸°í™”"""
        self.primary_llm = primary_llm
        self.fallback_llm = None
        self.factory = factory
        self.task = task
        self.callbacks = callbacks or []
        self.last_used = "primary"

        # F-14: ì¤‘ì•™í™”ëœ ì„¤ì •ì—ì„œ ì„±ëŠ¥ íŒŒë¼ë¯¸í„° ê°€ì ¸ì˜¤ê¸°
        try:
            from .centralized_settings import get_settings

            settings = get_settings()
            self.max_retries = settings.llm_max_retries
            self.retry_delay = settings.llm_retry_delay
            self.timeout = settings.llm_request_timeout

            # F-14: í…ŒìŠ¤íŠ¸ ëª¨ë“œ ê°ì§€ ë° ì„¤ì •
            self.test_mode = getattr(settings, "test_mode", False)
            self.mock_responses = getattr(settings, "mock_api_responses", False)
            self.skip_real_api = getattr(settings, "skip_real_api_calls", False)

            logger.info(
                f"F-14 LLM ì„¤ì • ë¡œë“œ: ì¬ì‹œë„={self.max_retries}, "
                f"ì§€ì—°={self.retry_delay}ì´ˆ, íƒ€ì„ì•„ì›ƒ={self.timeout}ì´ˆ, "
                f"í…ŒìŠ¤íŠ¸ëª¨ë“œ={self.test_mode}"
            )

        except Exception as e:
            # F-14 ì„¤ì • ë¡œë“œ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’ ì‚¬ìš©
            logger.warning(f"F-14 ì„¤ì • ë¡œë“œ ì‹¤íŒ¨, ê¸°ë³¸ê°’ ì‚¬ìš©: {e}")
            self.max_retries = 3
            self.retry_delay = 1.0
            self.timeout = 60

            # í…ŒìŠ¤íŠ¸ í™˜ê²½ ê°ì§€ (pytest ë˜ëŠ” í™˜ê²½ë³€ìˆ˜)
            import os
            import sys

            self.test_mode = "pytest" in sys.modules or os.getenv("TESTING") == "1"
            self.mock_responses = self.test_mode
            self.skip_real_api = self.test_mode

        logger.info(
            f"F-14 LLM ({task}) ì´ˆê¸°í™” ì™„ë£Œ: "
            f"íƒ€ì…={type(primary_llm).__name__}, "
            f"í…ŒìŠ¤íŠ¸ëª¨ë“œ={self.test_mode}, "
            f"ëª¨í‚¹={self.mock_responses}"
        )

        if self.test_mode:
            logger.info("F-14 í…ŒìŠ¤íŠ¸ ëª¨ë“œ í™œì„±í™” - ëª¨í‚¹ëœ ì‘ë‹µ ì‚¬ìš©")

    def invoke(self, input_data, config: Optional[RunnableConfig] = None, **kwargs):
        """F-14 ì¤‘ì•™í™”ëœ ì„¤ì •ì„ ì‚¬ìš©í•œ LLM í˜¸ì¶œ with fallback"""

        # F-14: í…ŒìŠ¤íŠ¸ ëª¨ë“œì—ì„œëŠ” ëª¨í‚¹ëœ ì‘ë‹µ ë°˜í™˜
        if self.test_mode and self.mock_responses:
            return self._generate_mock_response(input_data)

        # F-14: ì‹¤ì œ API í˜¸ì¶œ ê±´ë„ˆë›°ê¸° ì˜µì…˜
        if self.skip_real_api:
            logger.info("F-14: ì‹¤ì œ API í˜¸ì¶œ ê±´ë„ˆë›°ê¸°, ëª¨í‚¹ëœ ì‘ë‹µ ë°˜í™˜")
            return self._generate_mock_response(input_data)

        # ì‹¤ì œ LLM í˜¸ì¶œ (í”„ë¡œë•ì…˜ ëª¨ë“œ)
        return self._invoke_real_llm(input_data, config, **kwargs)

    def _generate_mock_response(self, input_data):
        """F-14 í…ŒìŠ¤íŠ¸ ëª¨ë“œìš© ëª¨í‚¹ëœ ì‘ë‹µ ìƒì„±"""
        logger.debug(f"F-14 í…ŒìŠ¤íŠ¸ ëª¨ë“œ: ëª¨í‚¹ëœ ì‘ë‹µ ìƒì„± ì¤‘...")

        # ì…ë ¥ ë°ì´í„° ê¸°ë°˜ ëª¨í‚¹ëœ ì‘ë‹µ ìƒì„±
        if isinstance(input_data, str):
            # ì‘ì—…ë³„ ë§ì¶¤í˜• ì‘ë‹µ
            if "í‚¤ì›Œë“œ" in input_data or "keyword" in input_data.lower():
                mock_content = "AI, ë¨¸ì‹ ëŸ¬ë‹, ë”¥ëŸ¬ë‹, ìì—°ì–´ì²˜ë¦¬, ì»´í“¨í„°ë¹„ì „"
            elif "ìš”ì•½" in input_data or "summary" in input_data.lower():
                mock_content = "AI ê¸°ìˆ ì´ ë¹ ë¥´ê²Œ ë°œì „í•˜ê³  ìˆìœ¼ë©°, ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ í™œìš©ë˜ê³  ìˆìŠµë‹ˆë‹¤."
            elif "ë²ˆì—­" in input_data or "translate" in input_data.lower():
                mock_content = "ì•ˆë…•í•˜ì„¸ìš”, ì„¸ê³„!"
            else:
                mock_content = (
                    f"F-14 í…ŒìŠ¤íŠ¸ ì‘ë‹µ: {input_data[:50]}ì— ëŒ€í•œ ëª¨í‚¹ëœ ê²°ê³¼ì…ë‹ˆë‹¤."
                )
        else:
            mock_content = (
                "F-14 í…ŒìŠ¤íŠ¸ ì‘ë‹µ: ì¤‘ì•™í™”ëœ ì„¤ì •ì„ ì‚¬ìš©í•œ ëª¨í‚¹ëœ LLM ì‘ë‹µì…ë‹ˆë‹¤."
            )

        # LangChain AIMessage í˜•íƒœë¡œ ë°˜í™˜
        try:
            from langchain_core.messages import AIMessage

            return AIMessage(content=mock_content)
        except ImportError:
            # Fallback: ê°„ë‹¨í•œ ê°ì²´ ë°˜í™˜
            class MockAIMessage:
                def __init__(self, content):
                    self.content = content

                def __str__(self):
                    return self.content

            return MockAIMessage(mock_content)

    def _invoke_real_llm(self, input_data, config=None, **kwargs):
        """ì‹¤ì œ LLM í˜¸ì¶œ (í”„ë¡œë•ì…˜ ëª¨ë“œ)"""
        # ê¸°ì¡´ retry ë¡œì§ ìœ ì§€
        max_retries = self.max_retries

        for attempt in range(max_retries + 1):
            try:
                logger.debug(f"LLM í˜¸ì¶œ ì‹œë„ {attempt + 1}/{max_retries + 1}")

                result = self.primary_llm.invoke(input_data, config=config, **kwargs)
                self.last_used = "primary"
                return result

            except Exception as e:
                if self._is_retryable_error(e) and attempt < max_retries:
                    delay = self.retry_delay
                    wait_time = delay * (2**attempt)  # Exponential backoff
                    logger.warning(f"LLM í˜¸ì¶œ ì‹¤íŒ¨, {wait_time}ì´ˆ í›„ ì¬ì‹œë„: {e}")
                    time.sleep(wait_time)
                    continue
                else:
                    if self.fallback_llm:
                        logger.warning(f"Primary LLM ì‹¤íŒ¨, fallback ì‚¬ìš©: {e}")
                        try:
                            result = self.fallback_llm.invoke(
                                input_data, config=config, **kwargs
                            )
                            self.last_used = "fallback"
                            return result
                        except Exception as fallback_error:
                            logger.error(f"Fallback LLMë„ ì‹¤íŒ¨: {fallback_error}")

                    logger.error(f"ëª¨ë“  LLM í˜¸ì¶œ ì‹¤íŒ¨: {e}")
                    raise e

    def stream(self, input_data, config: Optional[RunnableConfig] = None, **kwargs):
        """ìŠ¤íŠ¸ë¦¬ë° ì§€ì› (F-14 ê°œì„ )"""
        try:
            if hasattr(self.primary_llm, "stream"):
                return self.primary_llm.stream(input_data, config=config, **kwargs)
            else:
                # ìŠ¤íŠ¸ë¦¬ë°ì„ ì§€ì›í•˜ì§€ ì•ŠëŠ” ê²½ìš° ì¼ë°˜ invoke ì‚¬ìš©
                result = self.invoke(input_data, config=config, **kwargs)
                yield result
        except Exception as e:
            error_str = str(e).lower()
            is_retryable = any(
                keyword in error_str
                for keyword in [
                    "529",
                    "429",
                    "quota",
                    "rate limit",
                    "too many requests",
                    "overloaded",
                ]
            )

            if is_retryable:
                if self.fallback_llm is None:
                    self.fallback_llm = self._get_fallback_llm()

                if self.fallback_llm and hasattr(self.fallback_llm, "stream"):
                    return self.fallback_llm.stream(input_data, config=config, **kwargs)
                elif self.fallback_llm:
                    result = self.fallback_llm.invoke(
                        input_data, config=config, **kwargs
                    )
                    yield result
                else:
                    raise e
            else:
                raise e

    def batch(self, inputs, config: Optional[RunnableConfig] = None, **kwargs):
        """ë°°ì¹˜ ì²˜ë¦¬ ì§€ì› (F-14 ê°œì„ )"""
        try:
            if hasattr(self.primary_llm, "batch"):
                return self.primary_llm.batch(inputs, config=config, **kwargs)
            else:
                # ë°°ì¹˜ë¥¼ ì§€ì›í•˜ì§€ ì•ŠëŠ” ê²½ìš° ê°œë³„ ì²˜ë¦¬
                return [
                    self.invoke(input_data, config=config, **kwargs)
                    for input_data in inputs
                ]
        except Exception as e:
            error_str = str(e).lower()
            is_retryable = any(
                keyword in error_str
                for keyword in [
                    "529",
                    "429",
                    "quota",
                    "rate limit",
                    "too many requests",
                    "overloaded",
                ]
            )

            if is_retryable:
                if self.fallback_llm is None:
                    self.fallback_llm = self._get_fallback_llm()

                if self.fallback_llm and hasattr(self.fallback_llm, "batch"):
                    return self.fallback_llm.batch(inputs, config=config, **kwargs)
                elif self.fallback_llm:
                    return [
                        self.fallback_llm.invoke(input_data, config=config, **kwargs)
                        for input_data in inputs
                    ]
                else:
                    raise e
            else:
                raise e

    def _get_fallback_llm(self):
        """Fallback LLMì„ ì°¾ì•„ì„œ ë°˜í™˜"""
        primary_provider = type(self.primary_llm).__name__
        primary_model = getattr(self.primary_llm, "model", "unknown")

        logger.info(
            f"{primary_provider} ({primary_model})ì— ëŒ€í•œ ëŒ€ì²´ ëª¨ë¸ì„ ì°¾ëŠ” ì¤‘ì…ë‹ˆë‹¤"
        )

        # 1. ê°™ì€ ì œê³µì ë‚´ì—ì„œ ì•ˆì •ì ì¸ ëª¨ë¸ë¡œ fallback ì‹œë„
        if "gemini" in primary_provider.lower():
            stable_models = ["gemini-1.5-pro", "gemini-1.5-flash"]

            for stable_model in stable_models:
                if stable_model != primary_model:  # ë™ì¼í•œ ëª¨ë¸ì´ ì•„ë‹Œ ê²½ìš°ë§Œ
                    try:
                        logger.info(
                            f"ì•ˆì •ì ì¸ Gemini ëª¨ë¸ì„ ì‹œë„í•©ë‹ˆë‹¤: {stable_model}"
                        )
                        fallback_config = {
                            "provider": "gemini",
                            "model": stable_model,
                            "temperature": getattr(
                                self.primary_llm, "temperature", 0.3
                            ),
                            "max_retries": 2,
                            "timeout": 60,
                        }

                        # ë¹„ìš© ì¶”ì  ì½œë°± ì¶”ê°€
                        fallback_callbacks = list(self.callbacks)
                        try:
                            cost_callback = get_cost_callback_for_provider("gemini")
                            fallback_callbacks.append(cost_callback)
                        except Exception as e:
                            handle_exception(
                                e, "ë¹„ìš© ì¶”ì  ì½œë°± ì¶”ê°€", log_level=logging.INFO
                            )
                            # ë¹„ìš© ì¶”ì  ì‹¤íŒ¨ëŠ” ì¹˜ëª…ì ì´ì§€ ì•ŠìŒ

                        provider = self.factory.providers.get("gemini")
                        if provider and provider.is_available():
                            return provider.create_model(
                                fallback_config, fallback_callbacks
                            )
                    except Exception as e:
                        logger.warning(
                            f"ì•ˆì •ì ì¸ Gemini ëª¨ë¸ {stable_model} ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {e}"
                        )
                        continue

        # 2. ë‹¤ë¥¸ ì œê³µìë¡œ fallback ì‹œë„
        available_providers = self.factory.get_available_providers()

        for provider_name in available_providers:
            provider = self.factory.providers[provider_name]

            # ì´ë¯¸ ì‹œë„í•œ ì œê³µìëŠ” ìŠ¤í‚µ (ë‹¨, GeminiëŠ” ë‹¤ë¥¸ ëª¨ë¸ë¡œ ì´ë¯¸ ì‹œë„í–ˆìœ¼ë¯€ë¡œ ì œì™¸)
            if provider_name == "gemini":
                continue

            try:
                # ì œê³µìë³„ ì•ˆì •ì ì¸ ê¸°ë³¸ ëª¨ë¸ ì‚¬ìš©
                stable_model_map = {
                    "openai": "gpt-4o",
                    "anthropic": "claude-3-5-sonnet-20241022",
                }

                fallback_model = stable_model_map.get(
                    provider_name, self.factory._get_default_model(provider_name)
                )

                logger.info(
                    f"ë‹¤ë¥¸ ì œê³µìë¥¼ ì‹œë„í•©ë‹ˆë‹¤: {provider_name} (ëª¨ë¸: {fallback_model})"
                )

                fallback_config = {
                    "provider": provider_name,
                    "model": fallback_model,
                    "temperature": getattr(self.primary_llm, "temperature", 0.3),
                    "max_retries": 2,
                    "timeout": 60,
                }

                # ë¹„ìš© ì¶”ì  ì½œë°± ì¶”ê°€
                fallback_callbacks = list(self.callbacks)
                try:
                    cost_callback = get_cost_callback_for_provider(provider_name)
                    fallback_callbacks.append(cost_callback)
                except Exception as e:
                    handle_exception(
                        e,
                        f"ë¹„ìš© ì¶”ì  ì½œë°± ì¶”ê°€ ({provider_name})",
                        log_level=logging.INFO,
                    )
                    # ë¹„ìš© ì¶”ì  ì‹¤íŒ¨ëŠ” ì¹˜ëª…ì ì´ì§€ ì•ŠìŒ

                return provider.create_model(fallback_config, fallback_callbacks)

            except Exception as e:
                logger.warning(f"{provider_name}ìœ¼ë¡œ ëŒ€ì²´ LLM ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {e}")
                continue

        logger.warning("ëŒ€ì²´ LLMì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        return None

    def __getattr__(self, name):
        """ë‹¤ë¥¸ ì†ì„±ë“¤ì€ primary LLMì— ìœ„ì„"""
        return getattr(self.primary_llm, name)

    def _is_retryable_error(self, e):
        error_str = str(e).lower()
        return any(
            keyword in error_str
            for keyword in [
                "529",
                "429",
                "quota",
                "rate limit",
                "too many requests",
                "overloaded",
                "timeout",
                "connection",
            ]
        )


class LLMProvider(ABC):
    """LLM ì œê³µì ì¶”ìƒ í´ë˜ìŠ¤"""

    @abstractmethod
    def create_model(
        self, model_config: Dict[str, Any], callbacks: Optional[List] = None
    ):
        """LLM ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        pass

    @abstractmethod
    def is_available(self) -> bool:
        """ì œê³µìê°€ ì‚¬ìš© ê°€ëŠ¥í•œì§€ í™•ì¸í•©ë‹ˆë‹¤."""
        pass


class GeminiProvider(LLMProvider):
    """Google Gemini LLM ì œê³µì"""

    def create_model(
        self, model_config: Dict[str, Any], callbacks: Optional[List] = None
    ):
        """Gemini ëª¨ë¸ ìƒì„± - F-14 ì¤‘ì•™í™”ëœ ì„¤ì • ì‹œìŠ¤í…œ ì ìš©"""
        logger.debug("Creating Gemini model")

        if not ChatGoogleGenerativeAI:
            raise ImportError("langchain_google_genai íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")

        # GEMINI_API_KEY í™˜ê²½ë³€ìˆ˜ ì‚¬ìš© (GOOGLE_API_KEYì—ì„œ ë³€ê²½)
        api_key = os.getenv("GEMINI_API_KEY")
        if not api_key:
            raise ValueError("GEMINI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")

        # F-14: ì¤‘ì•™í™”ëœ ì„¤ì •ì—ì„œ íŒŒë¼ë¯¸í„° ê°€ì ¸ì˜¤ê¸°
        model_params = model_config.copy()
        if CENTRALIZED_SETTINGS_AVAILABLE:
            try:
                settings = get_settings()
                model_params.setdefault("timeout", settings.llm_request_timeout)
                # ë¹ ë¥¸ ëª¨ë“œ í™œì„±í™” ì‹œ ë” ë¹ ë¥¸ ëª¨ë¸ ì‚¬ìš©
                if settings.enable_fast_mode and "gemini-1.5-pro" in model_params.get(
                    "model", ""
                ):
                    model_params["model"] = "gemini-1.5-flash"
                    logger.info("ë¹ ë¥¸ ëª¨ë“œ: Gemini Proë¥¼ Flashë¡œ ë³€ê²½")
            except Exception as e:
                logger.warning(f"ì¤‘ì•™í™”ëœ ì„¤ì • ì ìš© ì‹¤íŒ¨, ê¸°ë³¸ê°’ ì‚¬ìš©: {e}")
                model_params.setdefault("timeout", 120)

        model_params.setdefault("temperature", 0.3)
        model_params.setdefault("max_tokens", 4000)
        model_params.setdefault("model", "gemini-1.5-pro")

        cost_callback = get_cost_callback_for_provider("gemini")
        all_callbacks = (callbacks or []) + ([cost_callback] if cost_callback else [])

        logger.debug(f"Gemini ëª¨ë¸ ìƒì„±: {model_params}")

        return ChatGoogleGenerativeAI(
            google_api_key=api_key,
            model=model_params["model"],
            temperature=model_params["temperature"],
            max_output_tokens=model_params["max_tokens"],
            timeout=model_params.get("timeout", 120),
            callbacks=all_callbacks,
        )

    def is_available(self) -> bool:
        # GEMINI_API_KEY í™˜ê²½ë³€ìˆ˜ ì‚¬ìš© (GOOGLE_API_KEYì—ì„œ ë³€ê²½)
        return ChatGoogleGenerativeAI is not None and bool(os.getenv("GEMINI_API_KEY"))


class OpenAIProvider(LLMProvider):
    """OpenAI LLM ì œê³µì"""

    def create_model(
        self, model_config: Dict[str, Any], callbacks: Optional[List] = None
    ):
        """OpenAI ëª¨ë¸ ìƒì„± - F-14 ì¤‘ì•™í™”ëœ ì„¤ì • ì‹œìŠ¤í…œ ì ìš©"""
        logger.debug("Creating OpenAI model")

        if not ChatOpenAI:
            raise ImportError("langchain_openai íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")

        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            raise ValueError("OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")

        # F-14: ì¤‘ì•™í™”ëœ ì„¤ì •ì—ì„œ íŒŒë¼ë¯¸í„° ê°€ì ¸ì˜¤ê¸°
        model_params = model_config.copy()
        if CENTRALIZED_SETTINGS_AVAILABLE:
            try:
                settings = get_settings()
                model_params.setdefault("timeout", settings.llm_request_timeout)
                # ë¹ ë¥¸ ëª¨ë“œ í™œì„±í™” ì‹œ ë” ë¹ ë¥¸ ëª¨ë¸ ì‚¬ìš©
                if settings.enable_fast_mode and "gpt-4" in model_params.get(
                    "model", ""
                ):
                    model_params["model"] = "gpt-3.5-turbo"
                    logger.info("ë¹ ë¥¸ ëª¨ë“œ: GPT-4ë¥¼ GPT-3.5-turboë¡œ ë³€ê²½")
            except Exception as e:
                logger.warning(f"ì¤‘ì•™í™”ëœ ì„¤ì • ì ìš© ì‹¤íŒ¨, ê¸°ë³¸ê°’ ì‚¬ìš©: {e}")
                model_params.setdefault("timeout", 120)

        model_params.setdefault("temperature", 0.3)
        model_params.setdefault("max_tokens", 4000)
        model_params.setdefault("model", "gpt-4o-mini")

        cost_callback = get_cost_callback_for_provider("openai")
        all_callbacks = (callbacks or []) + ([cost_callback] if cost_callback else [])

        logger.debug(f"OpenAI ëª¨ë¸ ìƒì„±: {model_params}")

        return ChatOpenAI(
            api_key=api_key,
            model=model_params["model"],
            temperature=model_params["temperature"],
            max_tokens=model_params["max_tokens"],
            timeout=model_params.get("timeout", 120),
            callbacks=all_callbacks,
        )

    def is_available(self) -> bool:
        return ChatOpenAI is not None and bool(os.getenv("OPENAI_API_KEY"))


class AnthropicProvider(LLMProvider):
    """Anthropic Claude LLM ì œê³µì"""

    def create_model(
        self, model_config: Dict[str, Any], callbacks: Optional[List] = None
    ):
        if not ChatAnthropic:
            raise ImportError("langchain_anthropic íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")

        api_key = os.getenv("ANTHROPIC_API_KEY")
        if not api_key:
            raise ValueError("ANTHROPIC_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")

        # F-14: ì¤‘ì•™í™”ëœ ì„¤ì •ì—ì„œ íƒ€ì„ì•„ì›ƒ ì ìš©
        model_params = model_config.copy()
        if CENTRALIZED_SETTINGS_AVAILABLE:
            try:
                settings = get_settings()
                model_params.setdefault("timeout", settings.llm_request_timeout)
                # ë¹ ë¥¸ ëª¨ë“œ í™œì„±í™” ì‹œ ë” ë¹ ë¥¸ ëª¨ë¸ ì‚¬ìš©
                if settings.enable_fast_mode and "claude-3-opus" in model_params.get(
                    "model", ""
                ):
                    model_params["model"] = "claude-3-haiku-20240307"
                    logger.info("ë¹ ë¥¸ ëª¨ë“œ: Claude Opusë¥¼ Haikuë¡œ ë³€ê²½")
            except Exception as e:
                logger.warning(f"ì¤‘ì•™í™”ëœ ì„¤ì • ì ìš© ì‹¤íŒ¨, ê¸°ë³¸ê°’ ì‚¬ìš©: {e}")
                model_params.setdefault("timeout", 120)

        model_params.setdefault("temperature", 0.1)
        model_params.setdefault("max_tokens", 4000)
        model_params.setdefault("model", "claude-3-haiku-20240307")

        cost_callback = get_cost_callback_for_provider("anthropic")
        all_callbacks = (callbacks or []) + ([cost_callback] if cost_callback else [])

        logger.debug(f"Anthropic ëª¨ë¸ ìƒì„±: {model_params}")

        return ChatAnthropic(
            anthropic_api_key=api_key,
            model=model_params["model"],
            temperature=model_params["temperature"],
            max_tokens=model_params["max_tokens"],
            timeout=model_params.get("timeout", 120),
            callbacks=all_callbacks,
        )

    def is_available(self) -> bool:
        return ChatAnthropic is not None and bool(os.getenv("ANTHROPIC_API_KEY"))


class LLMFactory:
    """LLM íŒ©í† ë¦¬ í´ë˜ìŠ¤ - ë‹¤ì–‘í•œ LLM ì œê³µìë¥¼ í†µí•© ê´€ë¦¬"""

    def __init__(self):
        # ì‹œì‘ ì‹œ API í‚¤ ìœ íš¨ì„± ê²€ì‚¬
        try:
            validate_api_keys()
        except Exception as e:
            logger.error(f"API í‚¤ ê²€ì‚¬ ì‹¤íŒ¨: {e}")
            # ê²€ì‚¬ ì‹¤íŒ¨í•´ë„ íŒ©í† ë¦¬ëŠ” ìƒì„±í•˜ë˜, ì‚¬ìš© ì‹œ ì˜¤ë¥˜ ë°œìƒ

        self.providers = {
            "gemini": GeminiProvider(),
            "openai": OpenAIProvider(),
            "anthropic": AnthropicProvider(),
        }
        self.llm_config = config.LLM_CONFIG

    def get_llm_for_task(
        self, task: str, callbacks: Optional[List] = None, enable_fallback: bool = True
    ):
        """
        íŠ¹ì • ì‘ì—…ì— ìµœì í™”ëœ LLM ëª¨ë¸ì„ ë°˜í™˜í•©ë‹ˆë‹¤.

        Args:
            task: ì‘ì—… ìœ í˜• (keyword_generation, theme_extraction, etc.)
            callbacks: LangChain ì½œë°± ë¦¬ìŠ¤íŠ¸
            enable_fallback: í• ë‹¹ëŸ‰ ì´ˆê³¼ ì‹œ ìë™ fallback í™œì„±í™” ì—¬ë¶€

        Returns:
            LLM ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ (fallback ê¸°ëŠ¥ í¬í•¨)
        """
        model_config = self.llm_config.get("models", {}).get(task)

        if not model_config:
            # ê¸°ë³¸ ì„¤ì • ì‚¬ìš©
            provider_name = self.llm_config.get("default_provider", "gemini")
            model_config = {
                "provider": provider_name,
                "model": self._get_default_model(provider_name),
                "temperature": 0.3,
                "max_retries": 2,
                "timeout": 60,
            }

        provider_name = model_config.get("provider", "gemini")
        provider = self.providers.get(provider_name)

        if not provider:
            raise ValueError(f"Unknown provider: {provider_name}")

        # ë¹„ìš© ì¶”ì  ì½œë°± ìë™ ì¶”ê°€
        final_callbacks = list(callbacks) if callbacks else []
        try:
            cost_callback = get_cost_callback_for_provider(provider_name)
            final_callbacks.append(cost_callback)
        except Exception as e:
            handle_exception(
                e,
                f"Warning: Failed to add cost tracking for {provider_name}",
                log_level=logging.INFO,
            )

        if not provider.is_available():
            # Fallback to available provider
            logger.warning(
                f"âš ï¸ {provider_name}ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ì–´ ë‹¤ë¥¸ ì œê³µìë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤"
            )

            # ì‚¬ìš© ê°€ëŠ¥í•œ ì œê³µì ì°¾ê¸° (ìš°ì„ ìˆœìœ„: gemini > openai > anthropic)
            fallback_priority = ["gemini", "openai", "anthropic"]
            fallback_provider = None
            fallback_name = None

            for fallback_candidate in fallback_priority:
                if (
                    fallback_candidate in self.providers
                    and self.providers[fallback_candidate].is_available()
                ):
                    fallback_provider = self.providers[fallback_candidate]
                    fallback_name = fallback_candidate
                    break

            if fallback_provider:
                logger.info(f"âœ… {provider_name} ëŒ€ì‹  {fallback_name}ì„ ì‚¬ìš©í•©ë‹ˆë‹¤")
                model_config = model_config.copy()
                model_config["provider"] = fallback_name
                model_config["model"] = self._get_default_model(fallback_name)

                # Fallback ì œê³µìì— ëŒ€í•œ ë¹„ìš© ì¶”ì  ì½œë°± ì—…ë°ì´íŠ¸
                final_callbacks = list(callbacks) if callbacks else []
                try:
                    cost_callback = get_cost_callback_for_provider(fallback_name)
                    final_callbacks.append(cost_callback)
                except Exception as e:
                    handle_exception(
                        e,
                        f"ë¹„ìš© ì¶”ì  ì¶”ê°€ ({fallback_name})",
                        log_level=logging.INFO,
                    )
                    # ë¹„ìš© ì¶”ì  ì‹¤íŒ¨ëŠ” ì¹˜ëª…ì ì´ì§€ ì•ŠìŒ

                llm = fallback_provider.create_model(model_config, final_callbacks)

                # Fallback ë˜í¼ ì ìš©
                if enable_fallback:
                    return LLMWithFallback(llm, self, task, final_callbacks)
                else:
                    return llm
            else:
                # ì‚¬ìš© ê°€ëŠ¥í•œ ì œê³µìê°€ ì—†ëŠ” ê²½ìš°
                available_providers = []
                for name, prov in self.providers.items():
                    if prov.is_available():
                        available_providers.append(name)

                error_msg = f"âŒ ì‚¬ìš© ê°€ëŠ¥í•œ LLM ì œê³µìê°€ ì—†ìŠµë‹ˆë‹¤!"
                if available_providers:
                    error_msg += f" (ì‚¬ìš© ê°€ëŠ¥: {', '.join(available_providers)})"
                else:
                    error_msg += " ë‹¤ìŒ API í‚¤ ì¤‘ í•˜ë‚˜ë¥¼ ì„¤ì •í•˜ì„¸ìš”: GEMINI_API_KEY, OPENAI_API_KEY, ANTHROPIC_API_KEY"

                logger.error(error_msg)
                raise ValueError(error_msg)

        llm = provider.create_model(model_config, final_callbacks)

        # Fallback ë˜í¼ ì ìš©
        if enable_fallback:
            return LLMWithFallback(llm, self, task, final_callbacks)
        else:
            return llm

    def _get_default_model(self, provider_name: str) -> str:
        """ì œê³µìë³„ ê¸°ë³¸ ëª¨ë¸ëª…ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
        provider_models = self.llm_config.get("provider_models", {})
        models = provider_models.get(provider_name, {})
        return models.get(
            "standard",
            {
                "gemini": "gemini-1.5-pro",
                "openai": "gpt-4o",
                "anthropic": "claude-3-sonnet-20240229",
            }.get(provider_name, "gemini-1.5-pro"),
        )

    def get_available_providers(self) -> List[str]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ì œê³µì ëª©ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
        return [
            name for name, provider in self.providers.items() if provider.is_available()
        ]

    def get_provider_info(self) -> Dict[str, Dict[str, Any]]:
        """ê° ì œê³µìì˜ ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ì™€ ëª¨ë¸ ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        info = {}
        for name, provider in self.providers.items():
            info[name] = {
                "available": provider.is_available(),
                "models": self.llm_config.get("provider_models", {}).get(name, {}),
            }
        return info


# ì „ì—­ íŒ©í† ë¦¬ ì¸ìŠ¤í„´ìŠ¤
llm_factory = LLMFactory()


def get_llm_for_task(
    task: str, callbacks: Optional[List] = None, enable_fallback: bool = True
):
    """
    í¸ì˜ í•¨ìˆ˜: íŠ¹ì • ì‘ì—…ì— ìµœì í™”ëœ LLM ëª¨ë¸ì„ ë°˜í™˜í•©ë‹ˆë‹¤.

    Args:
        task: ì‘ì—… ìœ í˜•
        callbacks: LangChain ì½œë°± ë¦¬ìŠ¤íŠ¸
        enable_fallback: í• ë‹¹ëŸ‰ ì´ˆê³¼ ì‹œ ìë™ fallback í™œì„±í™” ì—¬ë¶€

    Returns:
        LLM ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤
    """
    # ì‹±ê¸€í†¤ íŒ¨í„´ìœ¼ë¡œ LLMFactory ì¸ìŠ¤í„´ìŠ¤ ê´€ë¦¬
    if not hasattr(get_llm_for_task, "_factory"):
        get_llm_for_task._factory = LLMFactory()

    return get_llm_for_task._factory.get_llm_for_task(task, callbacks, enable_fallback)


def get_available_providers() -> List[str]:
    """í¸ì˜ í•¨ìˆ˜: ì‚¬ìš© ê°€ëŠ¥í•œ LLM ì œê³µì ëª©ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    # API í‚¤ ê²€ì‚¬ ì‹¤í–‰
    try:
        return validate_api_keys()
    except Exception as e:
        logger.error(f"API í‚¤ ê²€ì‚¬ ì‹¤íŒ¨: {e}")
        return []


def get_provider_info() -> Dict[str, Dict[str, Any]]:
    """í¸ì˜ í•¨ìˆ˜: ì œê³µì ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    factory = LLMFactory()
    return factory.get_provider_info()
