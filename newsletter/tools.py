"""
Newsletter Generator - Custom Tools
ì´ ëª¨ë“ˆì€ ë‰´ìŠ¤ë ˆí„° ìƒì„±ì„ ìœ„í•œ LangChain ë„êµ¬ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.
"""

import json
import os
import re
import time
import uuid
from typing import Any, Dict, List, Optional

import markdownify
import requests
from bs4 import BeautifulSoup
from langchain.prompts import PromptTemplate
from langchain.tools import tool
from langchain_core.output_parsers import StrOutputParser
from langchain_core.outputs import Generation, LLMResult
from langchain_core.tools import ToolException
from langchain_google_genai import ChatGoogleGenerativeAI
from rich.console import Console

from . import config
from .utils.logger import get_logger, show_collection_brief
from .utils.error_handling import handle_exception

# ë¡œê±° ì´ˆê¸°í™”
logger = get_logger()
console = Console()


@tool
def search_news_articles(keywords: str, num_results: int = 10) -> List[Dict]:
    """
    Search for news articles using the Serper.dev API for each keyword.

    Args:
        keywords: Comma-separated keywords to search for, like 'AI,Machine Learning'
        num_results: Number of results to return per keyword (default: 10, max: 20)
      Returns:
        A list of article dictionaries with 'title', 'url', 'snippet', 'source', and 'date' keys.
    """
    if not config.SERPER_API_KEY:
        raise ToolException("SERPER_API_KEY not found. Please set it in the .env file.")

    if num_results > 20:
        num_results = 20  # Limit to 20 results max

    individual_keywords = [kw.strip() for kw in keywords.split(",")]
    all_collected_articles = []
    keyword_article_counts = {}

    logger.info("\nStarting article collection process:")
    for keyword in individual_keywords:
        logger.info(f"Searching articles for keyword: '{keyword}'")
        # ë‰´ìŠ¤ ì „ìš© ì—”ë“œí¬ì¸íŠ¸ ì‚¬ìš©ìœ¼ë¡œ ë³€ê²½
        url = "https://google.serper.dev/news"

        # ë‰´ìŠ¤ ì „ìš© ì—”ë“œí¬ì¸íŠ¸ëŠ” ë‹¨ìˆœí•œ íŒŒë¼ë¯¸í„°ë§Œ í•„ìš”
        payload = json.dumps(
            {"q": keyword, "gl": "kr", "num": num_results}  # í•œêµ­ ì§€ì—­ ê²°ê³¼
        )

        headers = {
            "X-API-KEY": config.SERPER_API_KEY,
            "Content-Type": "application/json",
        }

        try:
            response = requests.request("POST", url, headers=headers, data=payload)
            response.raise_for_status()

            results = response.json()
            articles_for_keyword = []

            # ì—¬ëŸ¬ ê°€ëŠ¥í•œ ê²°ê³¼ ì»¨í…Œì´ë„ˆë¥¼ í™•ì¸í•˜ì—¬ ë°ì´í„° ì¶”ì¶œ
            containers = []

            # 1. 'news' í‚¤ í™•ì¸ (ë‰´ìŠ¤ ì—”ë“œí¬ì¸íŠ¸ì˜ ì£¼ìš” ì‘ë‹µ í˜•ì‹)
            if "news" in results:
                logger.info(f"Found 'news' results for keyword '{keyword}'")
                containers.extend(results["news"])

            # 2. 'topStories' í‚¤ë„ í™•ì¸ (ì¼ë¶€ ì‘ë‹µì— ì¡´ì¬í•  ìˆ˜ ìˆìŒ)
            if "topStories" in results:
                logger.info(f"Found 'topStories' results for keyword '{keyword}'")
                containers.extend(results["topStories"])

            # 3. 'organic' í‚¤ í™•ì¸ (fallback - ì¼ë°˜ ê²€ìƒ‰ ê²°ê³¼)
            if "organic" in results and not containers:
                logger.info(f"Found 'organic' results for keyword '{keyword}'")
                containers.extend(results["organic"])

            # ê²°ê³¼ ë¡œê¹…
            logger.info(f"Total container items found: {len(containers)}")

            # ë””ë²„ê¹…: ì‘ë‹µ êµ¬ì¡° í™•ì¸
            if not containers and results:
                logger.warning(
                    f"Warning: No result containers found. Available keys: {list(results.keys())}"
                )
                if len(results.keys()) <= 3:  # í‚¤ê°€ ì ìœ¼ë©´ ì „ì²´ êµ¬ì¡° í™•ì¸
                    logger.warning(
                        f"Response structure: {json.dumps(results, ensure_ascii=False)[:300]}..."
                    )

            # ê° ì•„ì´í…œ ì²˜ë¦¬
            for item_idx, item in enumerate(
                containers[: min(num_results, len(containers))]
            ):
                # ë””ë²„ê¹… ì •ë³´ (ì²« 3ê°œ í•­ëª©ë§Œ)
                if item_idx < 3:
                    logger.debug(
                        f"Debug: Item keys (index: {item_idx}): {list(item.keys())}"
                    )
                    raw_date_val = item.get("date")
                    raw_published_at_val = item.get("publishedAt")
                    logger.debug(
                        f"Debug: Date value: '{raw_date_val}' / PublishedAt: '{raw_published_at_val}'"
                    )
                # ê³µí†µ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                article = {
                    "title": item.get("title", "ì œëª© ì—†ìŒ"),
                    "url": item.get("link", ""),
                    "link": item.get("link", ""),  # í˜¸í™˜ì„±ì„ ìœ„í•´ linkë„ ì¶”ê°€
                    "snippet": item.get("snippet")
                    or item.get("description", "ë‚´ìš© ì—†ìŒ"),
                    "source": item.get("source", "ì¶œì²˜ ì—†ìŒ"),
                    "date": item.get("date") or item.get("publishedAt") or "ë‚ ì§œ ì—†ìŒ",
                }
                articles_for_keyword.append(article)

            if not articles_for_keyword:
                logger.warning(f"No articles could be parsed for keyword '{keyword}'.")

            num_found = len(articles_for_keyword)
            keyword_article_counts[keyword] = num_found
            logger.info(f"Found {num_found} articles for keyword: '{keyword}'")
            all_collected_articles.extend(articles_for_keyword)

        except requests.exceptions.RequestException as e:
            logger.error(
                f"Error fetching articles for keyword '{keyword}' from Serper.dev: {e}"
            )
            # Continue to next keyword if one fails
        except json.JSONDecodeError:
            logger.error(
                f"Error decoding JSON response for keyword '{keyword}' from Serper.dev. Response: {response.text}"
            )
            # Continue to next keyword

    # ê²€ìƒ‰ ê²°ê³¼ ê°„ê²° í‘œì‹œ
    total_collected = len(all_collected_articles)
    if keyword_article_counts and total_collected > 0:
        show_collection_brief(keyword_article_counts)
    elif total_collected > 0:
        logger.info(f"ğŸ“° ì´ {total_collected}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘ ì™„ë£Œ")
    else:
        logger.warning("âš ï¸  ìˆ˜ì§‘ëœ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤")

    return all_collected_articles


@tool
def fetch_article_content(url: str) -> Dict[str, Any]:
    """
    Fetch the full content of an article from its URL.

    Args:
        url: The URL of the article to fetch

    Returns:
        A dictionary with 'title', 'url', and 'content' keys
    """
    try:
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
        }

        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()

        soup = BeautifulSoup(response.content, "html.parser")

        # íƒ€ì´í‹€ ì¶”ì¶œ
        title = soup.title.text.strip() if soup.title else "ì œëª© ì—†ìŒ"

        # ë©”ì¸ ì½˜í…ì¸  ì¶”ì¶œ ì‹œë„ (ë‹¤ì–‘í•œ ë°©ë²•ìœ¼ë¡œ)
        content = ""

        # ë©”íƒ€ ì„¤ëª… ì¶”ì¶œ
        meta_desc = soup.find("meta", attrs={"name": "description"})
        if meta_desc and meta_desc.get("content"):
            content += meta_desc.get("content") + "\n\n"

        # article íƒœê·¸ ì°¾ê¸°
        article_tag = soup.find("article")
        if article_tag:
            # ë¶ˆí•„ìš”í•œ ìš”ì†Œ ì œê±°
            for tag in article_tag.find_all(
                ["script", "style", "nav", "footer", "aside"]
            ):
                tag.decompose()

            content += article_tag.get_text(separator="\n", strip=True)
        else:
            # ë³¸ë¬¸ìœ¼ë¡œ ì¶”ì •ë˜ëŠ” íƒœê·¸ë“¤ ì°¾ê¸°
            for tag_name in ["div", "section"]:
                for attr in ["id", "class"]:
                    for keyword in [
                        "content",
                        "article",
                        "main",
                        "body",
                        "entry",
                        "post",
                    ]:
                        main_content = soup.find(
                            tag_name,
                            {
                                attr: lambda x: (
                                    x and keyword in x.lower() if x else False
                                )
                            },
                        )
                        if main_content:
                            # ë¶ˆí•„ìš”í•œ ìš”ì†Œ ì œê±°
                            for tag in main_content.find_all(
                                ["script", "style", "nav", "footer", "aside"]
                            ):
                                tag.decompose()

                            content += main_content.get_text(separator="\n", strip=True)
                            break
                    if content:
                        break
                if content:
                    break

        # ë³¸ë¬¸ì„ ì°¾ì§€ ëª»í•œ ê²½ìš° ì „ì²´ í…ìŠ¤íŠ¸ ì‚¬ìš©
        if not content:
            # ë¶ˆí•„ìš”í•œ ìš”ì†Œ ì œê±°
            for tag in soup.find_all(["script", "style", "nav", "footer", "aside"]):
                tag.decompose()

            content = (
                soup.body.get_text(separator="\n", strip=True)
                if soup.body
                else "ë‚´ìš©ì„ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            )

        # ê²°ê³¼ ë°˜í™˜
        return {
            "title": title,
            "url": url,
            "content": content[:5000],  # ì»¨í…ì¸  ê¸¸ì´ ì œí•œ (í† í° ì ˆì•½)
        }

    except Exception as e:
        raise ToolException(f"Error fetching article content: {str(e)}")


@tool
def save_newsletter_locally(
    html_content: str, filename_base: str, output_format: str = "html"
) -> str:
    """
    Save newsletter content locally as HTML or Markdown.

    Args:
        html_content: HTML content of the newsletter
        filename_base: Base filename (without extension)
        output_format: Format to save as ('html' or 'md')

    Returns:
        Path to the saved file
    """
    if output_format not in ["html", "md"]:
        raise ToolException("Format must be 'html' or 'md'")

    try:
        # Clean HTML markers from content before saving
        cleaned_html_content = clean_html_markers(html_content)

        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„± (ì—†ëŠ” ê²½ìš°)
        output_dir = os.path.join(os.getcwd(), "output")
        os.makedirs(output_dir, exist_ok=True)

        # íŒŒì¼ ê²½ë¡œ ìƒì„±
        file_path = os.path.join(output_dir, f"{filename_base}.{output_format}")

        # ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ ë³€í™˜ (í•„ìš”í•œ ê²½ìš°)
        if output_format == "md":
            content = markdownify.markdownify(cleaned_html_content, heading_style="ATX")
        else:
            content = cleaned_html_content

        # íŒŒì¼ ì €ì¥
        with open(file_path, "w", encoding="utf-8") as f:
            f.write(content)
        return f"Newsletter saved locally as {filename_base}.{output_format} at {file_path}"

    except Exception as e:
        raise ToolException(f"Error saving newsletter locally: {e}")


# clean_html_markers í•¨ìˆ˜ëŠ” newsletter.html_utils ëª¨ë“ˆë¡œ ì´ë™í–ˆìŠµë‹ˆë‹¤.
from .html_utils import clean_html_markers


def generate_keywords_with_gemini(
    domain: str, count: int = 10, callbacks=None
) -> list[str]:
    """
    Generates high-quality trend keywords for a given domain using configured LLM.
    """
    try:
        if callbacks is None:
            callbacks = []
        if os.environ.get("ENABLE_COST_TRACKING"):
            try:
                from .cost_tracking import get_tracking_callbacks

                handle_exception(None, "ë¹„ìš© ì¶”ì  ì½œë°± ì¶”ê°€", log_level=logging.INFO)
                callbacks += get_tracking_callbacks()
            except Exception as e:
                handle_exception(e, "ë¹„ìš© ì¶”ì  ì½œë°± ì¶”ê°€", log_level=logging.INFO)
                # ë¹„ìš© ì¶”ì  ì‹¤íŒ¨ëŠ” ì¹˜ëª…ì ì´ì§€ ì•ŠìŒ

        # LLM íŒ©í† ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ í‚¤ì›Œë“œ ìƒì„±ì— ìµœì í™”ëœ ëª¨ë¸ ì‚¬ìš©
        try:
            from .llm_factory import get_llm_for_task

            llm = get_llm_for_task(
                "keyword_generation", callbacks, enable_fallback=True
            )
        except Exception as e:
            logger.warning(f"LLM factory failed, using fallback: {e}")
            # Fallback to stable Gemini model
            if not config.GEMINI_API_KEY:
                logger.error(
                    "GEMINI_API_KEY is not configured. Cannot generate keywords."
                )
                return []

            llm = ChatGoogleGenerativeAI(
                model="gemini-1.5-pro",  # ë” ì•ˆì •ì ì¸ ëª¨ë¸ë¡œ ë³€ê²½
                temperature=0.7,
                google_api_key=config.GEMINI_API_KEY,
                transport="rest",
                convert_system_message_to_human=True,
                callbacks=callbacks,
                timeout=60,
                max_retries=3,  # ì¬ì‹œë„ íšŸìˆ˜ ì¦ê°€
                disable_streaming=False,
            )

        prompt_template = PromptTemplate.from_template(
            """You are a news search expert highly skilled at identifying effective search queries for finding the latest news articles about a given field.

        Based on the [Field Information] provided below, please generate exactly {count} highly effective search keywords (search queries) that someone would use *right now* to find recent news or significant events within this field.

        **IMPORTANT:** The generated keywords must be in **Korean**.

        Keyword Generation Guidelines:
        1.  Each keyword should be in the form of a natural phrase (generally 2-4 words) suitable for use in a real news search engine.
        2.  Prioritize terms that cover recent or currently notable major events, individuals, companies/organizations, technological changes, or other timely developments within the given field.
        3.  Include keywords that can yield specific search results rather than overly general or broad single words.
        4.  Ensure the generated keywords are likely to appear in Korean news headlines or body text and have a high probability of being used in actual news searches by Korean speakers.
        5.  Ensure you provide exactly the requested number of keywords ({count}).

        [Field Information]:
        {domain}

        Generated Search Keyword List (in Korean):
        (Provide each keyword on a new line, without any numbering or bullet points. Include only the Korean keywords themselves.)
        """
        )

        chain = prompt_template | llm | StrOutputParser()

        logger.info(f"Generating keywords for '{domain}' using Google Gemini...")

        # ì‹¤í–‰ ë° ì‘ë‹µ ì²˜ë¦¬
        response_content = chain.invoke({"domain": domain, "count": count})
        logger.debug(f"Raw response from Gemini:\n{response_content}")

        # ì‘ë‹µ ì²˜ë¦¬
        keywords = []
        for line in response_content.split("\n"):
            if not line.strip():
                continue

            # ì•ì˜ ë²ˆí˜¸ ë° ë§ˆí¬ë‹¤ìš´ ë³¼ë“œ í‘œì‹œ ì œê±°
            clean_line = re.sub(r"^\d+\.?\s*", "", line.strip())
            clean_line = re.sub(r"\*\*(.+?)\*\*", r"\1", clean_line)

            # ê´„í˜¸ ì•ˆì˜ ì˜ì–´ ì„¤ëª… ì œê±° (ìˆëŠ” ê²½ìš°)
            clean_line = re.sub(r"\s*\(.+?\)\s*$", "", clean_line)

            if clean_line:
                keywords.append(clean_line)

        # í‚¤ì›Œë“œ í˜•ì‹ ì²˜ë¦¬
        final_keywords = keywords[:count]
        if len(final_keywords) < count and keywords:
            final_keywords = keywords

        if len(final_keywords) == 1 and "," in final_keywords[0]:
            final_keywords = [kw.strip() for kw in final_keywords[0].split(",")][:count]

        # í‚¤ì›Œë“œ íš¨ê³¼ì„± ê²€ì¦ (ì˜µì…˜)
        final_keywords = validate_and_refine_keywords(
            final_keywords, min_results_per_keyword=3, count=count
        )

        logger.info(f"ìµœì¢… í‚¤ì›Œë“œ ({len(final_keywords)}):")
        for i, kw in enumerate(final_keywords, 1):
            logger.info(f"  {i}. {kw}")

        return final_keywords

    except Exception as e:
        logger.error(f"Error generating keywords with Gemini: {e}")
        return []


def validate_and_refine_keywords(
    keywords: list[str], min_results_per_keyword: int = 3, count: int = 10
) -> list[str]:
    """ê° í‚¤ì›Œë“œë¡œ ê²€ìƒ‰í–ˆì„ ë•Œ ì¶©ë¶„í•œ ê²°ê³¼ê°€ ë‚˜ì˜¤ëŠ”ì§€ ê²€ì¦í•˜ê³ , ë¬¸ì œê°€ ìˆëŠ” í‚¤ì›Œë“œëŠ” ëŒ€ì²´í•©ë‹ˆë‹¤."""

    validated_keywords = []
    replacement_needed = []

    logger.info(f"\nê²€ì¦ ì¤‘: ê° í‚¤ì›Œë“œê°€ ì¶©ë¶„í•œ ë‰´ìŠ¤ ê²°ê³¼ë¥¼ ë°˜í™˜í•˜ëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤...")

    for keyword in keywords:
        try:
            # í‚¤ì›Œë“œë¡œ í…ŒìŠ¤íŠ¸ ê²€ìƒ‰ (invoke ë©”ì„œë“œ ì‚¬ìš©)
            test_results = search_news_articles.invoke(
                {"keywords": keyword, "num_results": 5}
            )

            if len(test_results) >= min_results_per_keyword:
                validated_keywords.append(keyword)
                logger.info(
                    f"[green]âœ“ '{keyword}': {len(test_results)}ê°œ ê²°ê³¼ í™•ì¸[/green]"
                )
            else:
                replacement_needed.append(keyword)
                logger.info(
                    f"[yellow]âœ— '{keyword}': ê²°ê³¼ ë¶€ì¡± ({len(test_results)}ê°œ)[/yellow]"
                )

        except Exception as e:
            logger.info(f"[red]! '{keyword}' ê²€ì¦ ì¤‘ ì˜¤ë¥˜: {e}[/red]")
            replacement_needed.append(keyword)

    # ëŒ€ì²´ í‚¤ì›Œë“œ ìƒì„±ì´ í•„ìš”í•œ ê²½ìš°
    if replacement_needed and validated_keywords:
        logger.info(
            f"[yellow]{len(replacement_needed)}ê°œ í‚¤ì›Œë“œì— ëŒ€í•œ ëŒ€ì²´ í‚¤ì›Œë“œ ìƒì„± ì¤‘...[/yellow]"
        )

        # ì´ ë¶€ë¶„ë„ ìˆ˜ì • í•„ìš” - domain ë³€ìˆ˜ê°€ í•¨ìˆ˜ ë‚´ì—ì„œ ì ‘ê·¼í•  ìˆ˜ ì—†ìŒ
        new_keywords = []  # ì„ì‹œ ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¡œ ëŒ€ì²´

        validated_keywords.extend(new_keywords)

    return validated_keywords[:count]  # ì›ë˜ ìš”ì²­í•œ ê°œìˆ˜ë§Œí¼ ë°˜í™˜


def extract_common_theme_from_keywords(keywords, api_key=None, callbacks=None):
    """Extracts a common theme from a list of keywords using configured LLM."""
    if not keywords:
        return "General News"

    # Check if any API keys are available before attempting LLM calls
    if not api_key:
        api_key = config.GEMINI_API_KEY

    # Also check for other API keys that might be used by LLM factory
    has_any_api_key = (
        api_key
        or getattr(config, "OPENAI_API_KEY", None)
        or getattr(config, "ANTHROPIC_API_KEY", None)
    )

    if not has_any_api_key:
        logger.warning(
            "API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤. í…Œë§ˆ ì¶”ì¶œì„ ìœ„í•œ ê°„ë‹¨í•œ ëŒ€ì²´ ë°©ë²•ì„ ì‚¬ìš©í•©ë‹ˆë‹¤."
        )
        return extract_common_theme_fallback(keywords)

    try:
        # LLM íŒ©í† ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ í…Œë§ˆ ì¶”ì¶œì— ìµœì í™”ëœ ëª¨ë¸ ì‚¬ìš©
        try:
            from langchain_core.messages import HumanMessage
            from langchain_core.output_parsers import StrOutputParser
            from langchain_core.prompts import PromptTemplate

            from .llm_factory import get_llm_for_task

            if callbacks is None:
                callbacks = []
            if os.environ.get("ENABLE_COST_TRACKING"):
                try:
                    from .cost_tracking import get_tracking_callbacks

                    callbacks += get_tracking_callbacks()
                except Exception:
                    pass

            llm = get_llm_for_task("theme_extraction", callbacks, enable_fallback=False)

            prompt_template = PromptTemplate.from_template(
                """ë‹¤ìŒ í‚¤ì›Œë“œë“¤ì˜ ê³µí†µ ë¶„ì•¼ë‚˜ ì£¼ì œë¥¼ í•œêµ­ì–´ë¡œ ì¶”ì¶œí•´ ì£¼ì„¸ìš”:
                {keywords}

                ì¶œë ¥ í˜•ì‹:
                - ê³µí†µ ë¶„ì•¼/ì£¼ì œë§Œ ê°„ê²°í•˜ê²Œ ë‹µë³€í•´ ì£¼ì„¸ìš” (3ë‹¨ì–´ ì´ë‚´)
                - ì„¤ëª…ì´ë‚˜ ë¶€ê°€ ì •ë³´ëŠ” í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”
                - ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ë‹µë³€í•´ ì£¼ì„¸ìš”"""
            )

            chain = prompt_template | llm | StrOutputParser()
            extracted_theme = chain.invoke({"keywords": ", ".join(keywords)})

            if len(extracted_theme) > 30:
                extracted_theme = extracted_theme[:30]

            return extracted_theme.strip()

        except Exception as e:
            logger.warning(
                f"LLM íŒ©í† ë¦¬ë¥¼ í†µí•œ í…Œë§ˆ ì¶”ì¶œì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ëŒ€ì²´ ë°©ë²•ì„ ì‚¬ìš©í•©ë‹ˆë‹¤: {e}"
            )
            # Check if API key is available before trying Gemini fallback
            if not api_key:
                logger.warning(
                    "GEMINI_API_KEYë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í…Œë§ˆ ì¶”ì¶œì„ ìœ„í•œ ê°„ë‹¨í•œ ëŒ€ì²´ ë°©ë²•ì„ ì‚¬ìš©í•©ë‹ˆë‹¤."
                )
                return extract_common_theme_fallback(keywords)

        # Fallback using LangChain Google GenAI
        from langchain_core.messages import HumanMessage

        from .llm_factory import get_llm_for_task

        try:
            llm = get_llm_for_task("theme_extraction", callbacks, enable_fallback=True)

            final_prompt = f"""
ë‹¤ìŒ í‚¤ì›Œë“œë“¤ì˜ ê³µí†µ ë¶„ì•¼ë‚˜ ì£¼ì œë¥¼ í•œêµ­ì–´ë¡œ ì¶”ì¶œí•´ ì£¼ì„¸ìš”:
{', '.join(keywords)}

ì¶œë ¥ í˜•ì‹:
- ê³µí†µ ë¶„ì•¼/ì£¼ì œë§Œ ê°„ê²°í•˜ê²Œ ë‹µë³€í•´ ì£¼ì„¸ìš” (3ë‹¨ì–´ ì´ë‚´)
- ì„¤ëª…ì´ë‚˜ ë¶€ê°€ ì •ë³´ëŠ” í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”
- ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ë‹µë³€í•´ ì£¼ì„¸ìš”
"""

            response = llm.invoke([HumanMessage(content=final_prompt)])
            extracted_theme = response.content.strip()

            if len(extracted_theme) > 30:  # Keep the length check
                extracted_theme = extracted_theme[:30]

            return extracted_theme

        except Exception as llm_error:
            logger.warning(f"LLM factoryë¥¼ í†µí•œ í…Œë§ˆ ì¶”ì¶œì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {llm_error}")
            return extract_common_theme_fallback(keywords)

    except Exception as e:
        logger.error(f"í…Œë§ˆ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return extract_common_theme_fallback(keywords)


def extract_common_theme_fallback(keywords):
    """
    AI API ì—†ì´ ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹± ë°©ì‹ìœ¼ë¡œ ê³µí†µ ì£¼ì œ ì¶”ì¶œì„ ì‹œë„í•©ë‹ˆë‹¤.
    """
    if isinstance(keywords, str):
        keywords = [k.strip() for k in keywords.split(",") if k.strip()]

    if len(keywords) <= 1:
        return keywords[0] if keywords else ""

    # ê³µí†µ ë„ë©”ì¸ì„ ì°¾ì§€ ëª»í•œ ê²½ìš° - ë‹¨ìˆœí•œ í‚¤ì›Œë“œ ì¡°í•© ìš°ì„ 
    if len(keywords) <= 3:
        return ", ".join(keywords)
    else:
        # 4ê°œ ì´ìƒì¼ ë•ŒëŠ” "ì²«ë²ˆì§¸í‚¤ì›Œë“œ ì™¸ (ì´ê°œìˆ˜-1)ê°œ ë¶„ì•¼" í˜•ì‹
        return f"{keywords[0]} ì™¸ {len(keywords)-1}ê°œ ë¶„ì•¼"


def sanitize_filename(text):
    """
    íŒŒì¼ëª…ì— ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ í…ìŠ¤íŠ¸ë¥¼ ì •ë¦¬í•©ë‹ˆë‹¤.

    Args:
        text: ì •ë¦¬í•  í…ìŠ¤íŠ¸

    Returns:
        íŒŒì¼ëª…ì— ì í•©í•œ ë¬¸ìì—´
    """
    if not text:
        return "unknown"

    # 1. íŒŒì¼ëª…ì— í—ˆìš©ë˜ì§€ ì•ŠëŠ” íŠ¹ìˆ˜ ë¬¸ì ì œê±° ë˜ëŠ” ëŒ€ì²´
    invalid_chars = r'[\\/*?:"<>|]'
    text = re.sub(invalid_chars, "", text)

    # 2. ê´„í˜¸ë¥¼ ì œê±°í•˜ê³  ë‚´ìš©ë§Œ ìœ ì§€
    text = re.sub(r"\(([^)]*)\)", r"\1", text)

    # 3. ê³µë°±ì„ ì–¸ë”ìŠ¤ì½”ì–´ë¡œ ë³€ê²½
    text = text.replace(" ", "_")

    # 4. ì½¤ë§ˆ, ë§ˆì¹¨í‘œ ë“± ì¶”ê°€ ë¬¸ì ì²˜ë¦¬
    text = text.replace(",", "")
    text = text.replace(".", "")

    # 5. ì—°ì†ëœ ì–¸ë”ìŠ¤ì½”ì–´ ì²˜ë¦¬
    text = re.sub(r"_{2,}", "_", text)

    # 6. íŒŒì¼ëª… ê¸¸ì´ ì œí•œ (ìµœëŒ€ 50ì)
    if len(text) > 50:
        # ê¸´ ë‚´ìš© ì²˜ë¦¬: ë°©ë²• 1 - ë‹¨ì–´ ë‹¨ìœ„ë¡œ ì œí•œ
        words = text.split("_")
        if len(words) > 3:
            result = "_".join(words[:3]) + "_etc"
            # ê²°ê³¼ê°€ ì—¬ì „íˆ 50ì ì´ˆê³¼ì¸ ê²½ìš°
            if len(result) > 50:
                result = result[:46] + "_etc"
            return result
        else:
            # ë°©ë²• 2 - ê¸€ì ìˆ˜ ì§ì ‘ ì œí•œ
            return text[:46] + "_etc"

    return text


def get_filename_safe_theme(keywords, domain=None):
    """
    í‚¤ì›Œë“œ ë˜ëŠ” ë„ë©”ì¸ì—ì„œ íŒŒì¼ëª…ì— ì•ˆì „í•œ í…Œë§ˆ ë¬¸ìì—´ì„ ìƒì„±í•©ë‹ˆë‹¤.

    Args:
        keywords: í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ ë˜ëŠ” ë¬¸ìì—´
        domain: ë„ë©”ì¸ (ìˆëŠ” ê²½ìš°)

    Returns:
        íŒŒì¼ëª…ì— ì í•©í•œ í…Œë§ˆ ë¬¸ìì—´
    """
    # 1. ë¨¼ì € ì ì ˆí•œ í…Œë§ˆë¥¼ ì„ íƒ
    if domain:
        theme = domain
    elif isinstance(keywords, list) and len(keywords) == 1:
        theme = keywords[0]
    elif (isinstance(keywords, list) and len(keywords) > 1) or (
        isinstance(keywords, str) and "," in keywords
    ):
        theme = extract_common_theme_from_keywords(keywords)
    else:
        theme = keywords if isinstance(keywords, str) else ""

    # 2. í…Œë§ˆë¥¼ íŒŒì¼ëª…ì— ì í•©í•˜ê²Œ ì •ë¦¬
    return sanitize_filename(theme)


def regenerate_section_with_gemini(section_title: str, news_links: list) -> list:
    """
    êµ¬ì„±ëœ LLMì„ ì‚¬ìš©í•˜ì—¬ ë‰´ìŠ¤ ë§í¬ ëª©ë¡ìœ¼ë¡œë¶€í„° ì„¹ì…˜ ìš”ì•½ë¬¸ì„ ì¬ìƒì„±í•©ë‹ˆë‹¤.

    Args:
        section_title: ì„¹ì…˜ ì œëª©
        news_links: ë‰´ìŠ¤ ë§í¬ ì •ë³´ ëª©ë¡ (title, url, source_and_date í¬í•¨)

    Returns:
        list: ìƒì„±ëœ ìš”ì•½ë¬¸ ë¬¸ë‹¨ ëª©ë¡
    """
    from . import config

    # LLM íŒ©í† ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ì„¹ì…˜ ì¬ìƒì„±ì— ìµœì í™”ëœ ëª¨ë¸ ì‚¬ìš©
    try:
        from langchain_core.messages import HumanMessage

        from .llm_factory import get_llm_for_task

        llm = get_llm_for_task("section_regeneration", enable_fallback=False)

        # ë‰´ìŠ¤ ë§í¬ ì •ë³´ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜
        news_links_text = ""
        for i, link in enumerate(news_links, 1):
            title = (
                str(link.get("title", "No Title")).replace("{", "{{").replace("}", "}}")
            )
            source = (
                str(link.get("source_and_date", "Unknown Source"))
                .replace("{", "{{")
                .replace("}", "}}")
            )
            url = str(link.get("url", "#")).replace("{", "{{").replace("}", "}}")

            news_links_text += f"ê¸°ì‚¬ {i}:\n"
            news_links_text += f"ì œëª©: {title}\n"
            news_links_text += f"ì¶œì²˜: {source}\n"
            news_links_text += f"URL: {url}\n\n"

        prompt = f"""
        ë‹¤ìŒì€ '{section_title}'ì— ê´€ë ¨ëœ ë‰´ìŠ¤ ê¸°ì‚¬ ëª©ë¡ì…ë‹ˆë‹¤:
        
        {news_links_text}
        
        ìœ„ ë‰´ìŠ¤ ê¸°ì‚¬ë“¤ì„ ë°”íƒ•ìœ¼ë¡œ '{section_title}'ì— ëŒ€í•œ ì¢…í•©ì ì¸ ìš”ì•½ë¬¸ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.
        
        ìš”êµ¬ì‚¬í•­:
        1. 1ê°œì˜ ë¬¸ë‹¨ìœ¼ë¡œ ë‚˜ëˆ„ì–´ ì‘ì„±í•´ì£¼ì„¸ìš”. ê° ë¬¸ë‹¨ì€ ìµœì†Œ 3-4ë¬¸ì¥ ì´ìƒìœ¼ë¡œ êµ¬ì„±í•´ì£¼ì„¸ìš”.
        2. ë¬¸ë‹¨ì€ ì£¼ìš” íŠ¸ë Œë“œë‚˜ ë™í–¥ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”.
        5. ì „ë¬¸ì ì´ê³  ê°ê´€ì ì¸ í†¤ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.
        6. í•œêµ­ì–´ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.
        7. ê° ë¬¸ë‹¨ì€ ë³„ë„ì˜ ë¬¸ìì—´ë¡œ ë°˜í™˜í•´ì£¼ì„¸ìš” (ë¦¬ìŠ¤íŠ¸ í˜•íƒœ).
        """

        response = llm.invoke([HumanMessage(content=prompt)])
        response_text = response.content.strip()

        # ë¬¸ë‹¨ìœ¼ë¡œ ë¶„ë¦¬
        paragraphs = [p.strip() for p in response_text.split("\n\n") if p.strip()]

        # ìµœì†Œ 3ê°œì˜ ë¬¸ë‹¨ í™•ë³´
        while len(paragraphs) < 3:
            paragraphs.append("ì¶”ê°€ ì •ë³´ê°€ í•„ìš”í•©ë‹ˆë‹¤.")

        # ìµœëŒ€ 3ê°œ ë¬¸ë‹¨ìœ¼ë¡œ ì œí•œ
        return paragraphs[:3]

    except Exception as e:
        logger.warning(
            f"LLM íŒ©í† ë¦¬ë¥¼ í†µí•œ ì„¹ì…˜ ì¬ìƒì„±ì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ëŒ€ì²´ ë°©ë²•ì„ ì‚¬ìš©í•©ë‹ˆë‹¤: {e}"
        )

    # Fallback using LangChain Google GenAI
    from langchain_core.messages import HumanMessage

    from .llm_factory import get_llm_for_task

    try:
        llm = get_llm_for_task("section_regeneration", enable_fallback=True)
    except Exception as e:
        raise ValueError(f"LLM factory ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

    # ë‰´ìŠ¤ ë§í¬ ì •ë³´ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜ - ìˆ˜ì •ëœ í˜•ì‹ìœ¼ë¡œ
    news_links_text = ""
    for i, link in enumerate(news_links, 1):
        title = str(link.get("title", "No Title")).replace("{", "{{").replace("}", "}}")
        source = (
            str(link.get("source_and_date", "Unknown Source"))
            .replace("{", "{{")
            .replace("}", "}}")
        )
        url = str(link.get("url", "#")).replace("{", "{{").replace("}", "}}")

        news_links_text += f"ê¸°ì‚¬ {i}:\n"
        news_links_text += f"ì œëª©: {title}\n"
        news_links_text += f"ì¶œì²˜: {source}\n"
        news_links_text += f"URL: {url}\n\n"

    # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    prompt = f"""
    ë‹¤ìŒì€ '{section_title}'ì— ê´€ë ¨ëœ ë‰´ìŠ¤ ê¸°ì‚¬ ëª©ë¡ì…ë‹ˆë‹¤:
    
    {news_links_text}
    
    ìœ„ ë‰´ìŠ¤ ê¸°ì‚¬ë“¤ì„ ë°”íƒ•ìœ¼ë¡œ '{section_title}'ì— ëŒ€í•œ ì¢…í•©ì ì¸ ìš”ì•½ë¬¸ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.
    
    ìš”êµ¬ì‚¬í•­:
    1. 3ê°œì˜ ë¬¸ë‹¨ìœ¼ë¡œ ë‚˜ëˆ„ì–´ ì‘ì„±í•´ì£¼ì„¸ìš”. ê° ë¬¸ë‹¨ì€ ìµœì†Œ 3-4ë¬¸ì¥ ì´ìƒìœ¼ë¡œ êµ¬ì„±í•´ì£¼ì„¸ìš”.
    2. ì²« ë²ˆì§¸ ë¬¸ë‹¨ì€ ì£¼ìš” íŠ¸ë Œë“œë‚˜ ë™í–¥ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”.
    3. ë‘ ë²ˆì§¸ ë¬¸ë‹¨ì€ ì£¼ìš” ì´ìŠˆë‚˜ êµ¬ì²´ì ì¸ ì‚¬ë¡€ë¥¼ ë‹¤ë£¨ì–´ì£¼ì„¸ìš”.
    4. ì„¸ ë²ˆì§¸ ë¬¸ë‹¨ì€ ì‹œì‚¬ì ì´ë‚˜ ì „ë§ì„ ì œì‹œí•´ì£¼ì„¸ìš”.
    5. ì „ë¬¸ì ì´ê³  ê°ê´€ì ì¸ í†¤ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.
    6. í•œêµ­ì–´ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.
    7. ê° ë¬¸ë‹¨ì€ ë³„ë„ì˜ ë¬¸ìì—´ë¡œ ë°˜í™˜í•´ì£¼ì„¸ìš” (ë¦¬ìŠ¤íŠ¸ í˜•íƒœ).
    """

    try:
        response = llm.invoke([HumanMessage(content=prompt)])
        response_text = response.content.strip()

        # ë¬¸ë‹¨ìœ¼ë¡œ ë¶„ë¦¬
        paragraphs = [p.strip() for p in response_text.split("\n\n") if p.strip()]

        # ìµœì†Œ 3ê°œì˜ ë¬¸ë‹¨ í™•ë³´
        while len(paragraphs) < 3:
            paragraphs.append("ì¶”ê°€ ì •ë³´ê°€ í•„ìš”í•©ë‹ˆë‹¤.")

        # ìµœëŒ€ 3ê°œ ë¬¸ë‹¨ìœ¼ë¡œ ì œí•œ
        return paragraphs[:3]

    except Exception as e:
        import traceback

        logger.error(f"LLMì„ ì‚¬ìš©í•œ ì½˜í…ì¸  ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        logger.debug(f"ì˜¤ë¥˜ ì„¸ë¶€ ì •ë³´: {traceback.format_exc()}")
        return [
            "ìš”ì•½ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
            "ì¶”ê°€ ì •ë³´ê°€ í•„ìš”í•©ë‹ˆë‹¤.",
            "ë” ìì„¸í•œ ë¶„ì„ì´ í•„ìš”í•©ë‹ˆë‹¤.",
        ]


def generate_introduction_with_gemini(
    newsletter_topic: str, section_titles: list
) -> str:
    """
    êµ¬ì„±ëœ LLMì„ ì‚¬ìš©í•˜ì—¬ ë‰´ìŠ¤ë ˆí„° ì£¼ì œì™€ ì„¹ì…˜ ì œëª©ì„ ê¸°ë°˜ìœ¼ë¡œ ì†Œê°œ ë©”ì‹œì§€ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

    Args:
        newsletter_topic: ë‰´ìŠ¤ë ˆí„° ì£¼ì œ
        section_titles: ì„¹ì…˜ ì œëª© ëª©ë¡

    Returns:
        str: ìƒì„±ëœ ì†Œê°œ ë©”ì‹œì§€
    """
    from . import config

    # LLM íŒ©í† ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ì†Œê°œ ìƒì„±ì— ìµœì í™”ëœ ëª¨ë¸ ì‚¬ìš©
    try:
        from langchain_core.messages import HumanMessage

        from .llm_factory import get_llm_for_task

        llm = get_llm_for_task("introduction_generation", enable_fallback=False)

        # ì„¹ì…˜ ì œëª©ì„ ë¬¸ìì—´ë¡œ ë³€í™˜
        safe_topic = str(newsletter_topic).replace("{", "{{").replace("}", "}}")
        section_titles_text = ""
        for i, title in enumerate(section_titles, 1):
            safe_title = str(title).replace("{", "{{").replace("}", "}}")
            section_titles_text += f"- {safe_title}\n"

        prompt = f"""
        ë‹¤ìŒì€ ë‰´ìŠ¤ë ˆí„°ì˜ ì£¼ì œì™€ í¬í•¨ëœ ì„¹ì…˜ ì œëª©ë“¤ì…ë‹ˆë‹¤:
        
        ë‰´ìŠ¤ë ˆí„° ì£¼ì œ: {safe_topic}
        
        ì„¹ì…˜ ì œëª©:
        {section_titles_text}
        
        ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‰´ìŠ¤ë ˆí„°ì˜ ì†Œê°œ ë©”ì‹œì§€ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.
        
        ìš”êµ¬ì‚¬í•­:
        1. ì „ë¬¸ì ì´ê³  ì¹œì ˆí•œ í†¤ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.
        2. 2-3 ë¬¸ì¥ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ ì‘ì„±í•´ì£¼ì„¸ìš”.
        3. ì´ë²ˆ ë‰´ìŠ¤ë ˆí„°ì˜ ê°€ì¹˜ì™€ ì¤‘ìš”ì„±ì„ ê°•ì¡°í•´ì£¼ì„¸ìš”.
        4. í•œêµ­ì–´ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.
        5. ê° ì„¹ì…˜ì˜ í•µì‹¬ ë‚´ìš©ì´ ë¬´ì—‡ì¸ì§€ ê°„ëµíˆ ì–¸ê¸‰í•´ì£¼ì„¸ìš”.
        6. 'R&D ì „ëµ ìˆ˜ë¦½' ë˜ëŠ” 'ì˜ì‚¬ê²°ì •'ì— ë„ì›€ì´ ë  ìˆ˜ ìˆë‹¤ëŠ” ì ì„ ì–¸ê¸‰í•´ì£¼ì„¸ìš”.
        
        ì†Œê°œ ë©”ì‹œì§€ë§Œ ë°˜í™˜í•´ ì£¼ì„¸ìš”.
        """

        response = llm.invoke([HumanMessage(content=prompt)])
        return response.content.strip()

    except Exception as e:
        logger.warning(
            f"LLM íŒ©í† ë¦¬ë¥¼ í†µí•œ ì†Œê°œ ìƒì„±ì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ëŒ€ì²´ ë°©ë²•ì„ ì‚¬ìš©í•©ë‹ˆë‹¤: {e}"
        )
        # Fallback using LangChain Google GenAI
        from .llm_factory import get_llm_for_task

        try:
            llm = get_llm_for_task("introduction_generation", enable_fallback=True)
        except Exception as e:
            raise ValueError(f"LLM factory ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

    # ì„¹ì…˜ ì œëª©ì„ ë¬¸ìì—´ë¡œ ë³€í™˜
    safe_topic = str(newsletter_topic).replace("{", "{{").replace("}", "}}")
    section_titles_text = ""
    for i, title in enumerate(section_titles, 1):
        safe_title = str(title).replace("{", "{{").replace("}", "}}")
        section_titles_text += f"- {safe_title}\n"

    # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    prompt = f"""
    ë‹¤ìŒì€ ë‰´ìŠ¤ë ˆí„°ì˜ ì£¼ì œì™€ í¬í•¨ëœ ì„¹ì…˜ ì œëª©ë“¤ì…ë‹ˆë‹¤:
    
    ë‰´ìŠ¤ë ˆí„° ì£¼ì œ: {safe_topic}
    
    ì„¹ì…˜ ì œëª©:
    {section_titles_text}
    
    ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‰´ìŠ¤ë ˆí„°ì˜ ì†Œê°œ ë©”ì‹œì§€ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.
    
    ìš”êµ¬ì‚¬í•­:
    1. ì „ë¬¸ì ì´ê³  ì¹œì ˆí•œ í†¤ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.
    2. 2-3 ë¬¸ì¥ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ ì‘ì„±í•´ì£¼ì„¸ìš”.
    3. ì´ë²ˆ ë‰´ìŠ¤ë ˆí„°ì˜ ê°€ì¹˜ì™€ ì¤‘ìš”ì„±ì„ ê°•ì¡°í•´ì£¼ì„¸ìš”.
    4. í•œêµ­ì–´ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.
    5. ê° ì„¹ì…˜ì˜ í•µì‹¬ ë‚´ìš©ì´ ë¬´ì—‡ì¸ì§€ ê°„ëµíˆ ì–¸ê¸‰í•´ì£¼ì„¸ìš”.
    6. 'R&D ì „ëµ ìˆ˜ë¦½' ë˜ëŠ” 'ì˜ì‚¬ê²°ì •'ì— ë„ì›€ì´ ë  ìˆ˜ ìˆë‹¤ëŠ” ì ì„ ì–¸ê¸‰í•´ì£¼ì„¸ìš”.
    
    ì†Œê°œ ë©”ì‹œì§€ë§Œ ë°˜í™˜í•´ ì£¼ì„¸ìš”.
    """

    try:
        response = llm.invoke([HumanMessage(content=prompt)])
        introduction = response.content.strip()

        return introduction
    except Exception as e:
        import traceback

        logger.error(f"LLMì„ ì‚¬ìš©í•œ ì†Œê°œ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        logger.debug(f"ì˜¤ë¥˜ ì„¸ë¶€ ì •ë³´: {traceback.format_exc()}")
        return f"ê¸ˆì£¼ {safe_topic} ê´€ë ¨ ìµœì‹  ë™í–¥ê³¼ ì£¼ìš” ë‰´ìŠ¤ë¥¼ ì •ë¦¬í•˜ì—¬ ì œê³µí•©ë‹ˆë‹¤. ë³¸ ë‰´ìŠ¤ë ˆí„°ê°€ ì—…ë¬´ì— ë„ì›€ì´ ë˜ê¸°ë¥¼ ë°”ëë‹ˆë‹¤."
